\lecmargin{1}

{\bf \large What is Complex Analysis?} The main object of study is a \cdef{holomorphic} function $f: G \to \cc$, where $G \subseteq \cc$. Namely, a function for which the limit
\[\lim_{h\to 0}\frac{f(z+h) - f(z)}{h}\]
exists and is finite on an open set; that is, a \cdef{complex\text{-}differentiable\  function} on an open set. As a set, $\cc = \rr^2$, so one can naively expect the theory to be similar to that of real analysis, in this case the behaviour of differentiable functions. Interestingly, the requirement of holomorphicity can yield results that have no counterpart in the real case.\\
\\
A prime example of this is \emph{Louiville's Theorem. Every bounded holomorphic function is constant.}

\medskip

\begin{discussion}
We begin with first addressing the existence and nature of $\cc$ itself. Let $\rr$ denote the (field of) real numbers. One immediately deduces that the equation
\begin{equation*}\label{imaginary}
x^2 + 1 = 0\tag{$*$}
\end{equation*}
has no solution in the real numbers. The (field of) complex numbers $\cc$ stems from our desire to find a set containing $\rr$ that extends the algebraic operations of addition and multiplication of real numbers and which contains not only solutions to the polynomial equation above but solutions to all polynomial equations.\\[0.5em]
Surprisingly enough, the construction amounts to defining a symbol $i$ that is a solution to (\ref{imaginary}) and then considering all expressions of the form
\[x + iy,\quad x,y \in \rr\]
\end{discussion}

\bigskip

\section{Part I. Preliminaries}
\subsection{Construction of the (field of) Complex Numbers}
%\begin{mdframed}[backgroundcolor=paleyellow,linewidth=1pt]
%\begin{center}
%\section*{\sc\Large Part I. Preliminaries}
%\end{center}
%\end{mdframed}
%
%\begin{mdframed}
%\begin{center}
%\subsection*{\Large Construction of the (field of) Complex Numbers}
%\end{center}
%\end{mdframed}

\begin{definition}[The set of Complex Numbers]
A \cdef{complex\ number} $z$ is simply an order pair $z \coloneqq (x,y)$ of real numbers. Thus, the set of all complex numbers is given by
\[\cc \coloneqq \rr^2 = \setp{(x,y)}{x,y\in \rr}\]
If $z = (x,y)$ is a complex number, then we call 
\[\Re z \coloneqq x \quad \text{and} \quad \Im z \coloneqq y\]
the \cdef{real} and \cdef{imaginary\ parts} of $z$ respectively.\\[0.5em]
Two complex numbers $z_1$ and $z_2$ are equal if and only if $\Re z_1 = \Re z_2$ and $\Im z_1 = \Im z_2$.\\
\\
If $\Re z = 0$ and $\Im z \neq 0$, we say that $z$ is \cdef{purely\ imaginary}. The set of purely imaginary complex numbers corresponds to the $y$-axis and is called the \cdef{imaginary\ axis} in $\cc$.
\end{definition}

\medskip

\begin{definition}[Binary Operations on $\cc$]
Let $z_1 = (x_1,y_1)$ and $z_2 = (x_2,y_2)$ be complex numbers. Then their \emph{sum} is
\[z_1 + z_2 = (x_1,y_1) + (x_2,y_2) \coloneqq (x_1 + x_2,y_1 + y_2)\]
and their \emph{product} is 
\[z_1 \cdot z_2 = (x_1,y_1) \cdot (x_2,y_2) \coloneqq (x_1x_2 - y_1y_2, x_1y_2 + x_2y_1)\]
\end{definition}

\medskip

\begin{proposition}
There exists a subset of $\cc$ that is algebraically indistinguishable from $\rr$.
\end{proposition}
\begin{proof}
Consider the set (the $x$-axis)
\[\rr \times \set{0} = \setp{(x,0)}{x\in \rr} \subseteq \cc.\]
There is a bijection
\[\phi:\rr \to \rr \times \set{0},\ x \mapsto (x,0).\]
Moreover, 
\begin{align*}
\phi(x) + \phi(y) &= (x,0) + (y,0) = (x+y,0) = \phi(x+y)\\[0.5em]
\phi(x)\cdot\phi(y) &= (x,0)\cdot (y,0) = (xy - 0\cdot 0,x\cdot 0 + y\cdot 0) = (xy,0) = \phi(xy)\\[-2.5em]
\end{align*}
\end{proof}
According to the proposition, the operations of addition and multiplication on complex numbers we have defined extend the operations of addition and multiplication of real numbers. We therefore call the $x$-axis, the \cdef{real\ axis}.

\medskip

\begin{discussion}
We identify each complex number $(x,0)$ with the corresponding real number $x$; more than that, abusing notation, we write
\[1 = (1,0)\quad \text{and} \quad (x,0) = x(1,0) = x\]
Now, define the \cdef{imaginary\ unit} $i \coloneqq (0,1)$. Then
\[i^2 = i\cdot i = (0,1)\cdot (0,1) = (0^2 - 1^2,1\cdot 0 + 1\cdot 0) = (-1,0) = -1.\]
Moreover, for any $z = (x,y) \in \cc$ we see that
\begin{align*}
z &= (x,y)\\[0.5em]
&= (x,0) + y(0,1) = x + iy = \Re z + i\Im z
\end{align*}
Hence, with our new notation
\[\cc = \setp{x+iy}{x,y\in \rr,\ i^2 = -1}\]
and
\begin{align*}
z_1 + z_2 = (x_1 + iy_1) + (x_2 + iy_2) &= (x_1 + x_2) + i(y_1 + y_2)\\[0.5em]
z_1 \cdot z_2 = (x_1 + iy_1) \cdot (x_2 + iy_2) &= (x_1x_2 - y_1y_2) + i(x_1y_2 + x_2y_1)
\end{align*}\\
Although we have expanded the real numbers and we will see that the complex numbers have several new and familiar properties. We do end up losing one property of the real numbers when working with complex numbers: total ordering (that extends the one on $\rr$ or is compatible with multiplication). In the world of complex numbers, it no longer makes sense to ask if $z_1 > z_2$ (see Problem \ref{prob 1.6}).
\end{discussion}

\medskip

In practice, the product of complex numbers can be computed by multiplying the expressions as if they were polynomials in the variable $i$, and using $i^2 = -1$. The fact that this works is left as Problem \ref{prob 1.2}.
\begin{example}
Compute $(1+i)(1-3i)$.
\end{example}
\begin{proof}[Answer]
We note
\begin{align*}
(1 + i)(1 - 3i) &= (1 - 3i) + i(1 - 3i)\\[0.5em]
&= (1 - 3i) + (i - 3i^2)\\[0.5em]
&= (1 - 3i) + (i + 3) = 4 - 2i\\[-2.5em]
\end{align*}
\end{proof}

\medskip

\begin{proposition}[Algebraic Properties of $(\cc,+,\ \cdot\ )$]\label{cafield}\lecmargin{2}
\hfill
\begin{itemize}
\item[(1)] \emph{Additive Identity.} For every $z \in \cc$
\[z + 0 = z = 0 + z\]
\item[(2)] \emph{Associativity of Addition.} For every triple $z_1,z_2,z_3 \in \cc$
\[z_1 + (z_2 + z_3) = (z_1 + z_2) + z_3\]
\item[(3)] \emph{Commutativity of Addition.} For every pair $z_1,z_2 \in \cc$
\[z_1 + z_2 = z_2 + z_1\]
\item[(4)] \emph{Additive Inverses.} For every $z \in \cc$, there exists a complex number, denoted $-z$, such that
\[z + (-z) = 0 = (-z) + z\]
In fact, $-z \coloneqq (-1)z$, which is described in Problem \ref{prob 1.1}.
\item[(5)] \emph{Multiplicative Identity.} For every $z \in \cc$
\[z \cdot 1 = z = 1 \cdot z\]
\item[(6)] \emph{Associativity of Multiplication.} For every triple $z_1,z_2,z_3 \in \cc$
\[z_1 \cdot (z_2 \cdot z_3) = (z_1 \cdot z_2) \cdot z_3\]
\item[(7)] \emph{Commutativity of Multiplication.} For every pair $z_1,z_2 \in \cc$
\[z_1\cdot z_2 = z_2\cdot z_1\]
\item[(8)] \emph{Multiplicative Inverses.} For every $z \in \cc^* \coloneqq \cc\setminus\set{0}$, there exists a complex number, denoted $z^{-1}$ or $1/z$, such that
\[z \cdot z^{-1} = 1 = z^{-1} \cdot z\]
In fact, if $z =  x + iy$, then $z^{-1} = \dfrac{1}{z} \coloneqq \dfrac{x}{x^2 + y^2} - i\ \dfrac{y}{x^2 + y^2}$.
\item[(9)] \emph{Distributive Law.} For every triple $z_1,z_2,z_3 \in \cc$
\[(z_1 + z_2)\cdot z_3 = z_1\cdot z_3 + z_2\cdot z_3\]
\end{itemize}
\end{proposition}
\begin{proof}
(1) - (7) and (9) are left as Problem \ref{prob 1.3}. One proves these directly by showing that the left hand side matches the right hand side.
\begin{itemize}
\item[(8)] We note that
\begin{align*}
z\cdot\frac{1}{z} &= (x + iy)\left(\frac{x}{x^2 + y^2} - i\ \frac{y}{x^2 + y^2}\right)\\[1em]
&= (x + iy)\left(\frac{x}{x^2 + y^2} + i\ \frac{(-y)}{x^2 + y^2}\right)\\[1em]
&= \left(x\cdot\frac{x}{x^2 + y^2} - y\cdot\frac{(-y)}{x^2 + y^2}\right) + i\left(x\cdot\frac{(-y)}{x^2 + y^2} + y\cdot\frac{x}{x^2 + y^2}\right)\\[1em]
&= \left(\frac{x^2}{x^2 + y^2} + \frac{y^2}{x^2 + y^2}\right) + i\left(\frac{-yx + xy}{x^2 + y^2}\right)\\[1em]
&= \frac{x^2 + y^2}{x^2 + y^2} + i\cdot 0\\[1em]
&= 1
\end{align*}
Of course, we should comment that $z = (x,y) \neq (0,0)$ if and only if $x^2 + y^2 \neq 0$ (one proves this by stating and proving the contrapositive).
\end{itemize}
\vspace*{-\baselineskip}
\end{proof}

\vspace*{1.5em}

\begin{remark}
In the language of algebra, 
\begin{itemize}[leftmargin=*]
\item (1) -- (4) tells us that $(\cc,+)$ is an abelian group.
\item (5) -- (8) tells us that $(\cc^*,\ \cdot\ )$ is an abelian group.
\item (1) -- (9) tells us that $(\cc,+,\ \cdot\ )$ is a field.
\end{itemize}
\end{remark}

\vspace*{1.5em}

\begin{definition}
Consider $z_1,z_2 \in \cc$. We define \emph{subtraction} and \emph{division} as follows, respectively:
\begin{align*}
z_1 - z_2 &\coloneqq z_1 + (-z_2)\\[0.5em]
\dfrac{z_1}{z_2} &\coloneqq z_1\cdot z_2^{-1} = z_1\cdot \left(\dfrac{1}{z_2}\right),\quad z_2 \neq 0
\end{align*}
Writing down $z_1/z_2$ as $x+ iy$ is not easy to remember, one obtains it by a method akin to "rationalising the denominator", in this case we could call it "realifiying the denominator"
\[\frac{z_1}{z_2} = \frac{x_1 + iy_1}{x_2 + iy_2}\cdot \frac{x_2 - iy_2}{x_2 - iy_2}\]
This method will be clarified soon when we talk about conjugates and absolute value.
\end{definition}

%\bigskip

\subsection{Geometric Properties of Complex Numbers}
%\begin{mdframed}
%\begin{center}
%{\Large Geometric Properties of Complex Numbers}
%\end{center}
%\end{mdframed}

As a set, we have $\cc = \rr^2$, so it's natural to visualise complex numbers as points in the \cdef{complex\ plane} (also called the \cdef{Argand\ plane}).
\[\begin{tikzpicture}[scale=0.75]
    \draw[<->,thick] (-1,0)--(5.5,0) node[right]{Real Axis};
	\draw[<->,thick] (0,-1)--(0,5.5) node[above]{Imaginary Axis};
	\draw[dashed] (4,3.5)--(0,3.5);
	\draw[dashed] (4,3.5)--(4,0);
    \fill (4,3.5) circle (2pt) node[right]{\ $z = x+iy$};
    \fill (0,3.5) circle (2pt) node[left]{$y$};
    \fill (4,0) circle (2pt) node[below]{$x$};
    \draw[->,>=stealth,thick] (0,0) -- (4,3.5);
  \end{tikzpicture}\]
Geometrically, addition of complex numbers is just the addition of the corresponding vectors in the euclidean plane. We will soon see a geometric interpretation of multiplication.

\[\begin{tikzpicture}[scale=0.75]
    \draw[<->,thick] (-1,0)--(5.5,0) node[right]{Real Axis};
	\draw[<->,thick] (0,-1)--(0,5.5) node[above]{Imaginary Axis};
    \fill[newblue] (6,4.5) circle (2pt) node[right]{\ $z_1 + z_2$};
    \fill (1,3.5) circle (2pt) node[above]{$z_1$};
    \fill (5,1) circle (2pt) node[right]{$z_2$};
    \draw[->,>=stealth,thick] (0,0) -- (1,3.5);
    \draw[->,>=stealth,thick] (0,0) -- (5,1);
	\draw[thick,dashed,newblue] (1,3.5)--(6,4.5);
	\draw[thick,dashed,newblue] (5,1)--(6,4.5);
    \draw[->,>=stealth,thick,newblue] (0,0) -- (6,4.5);
  \end{tikzpicture}\]

\medskip
 
\begin{definition}[Modulus]\label{cmplxnorm}
The \cdef{modulus} (or \cdef{absolute\ value}) of a complex number $z = x + iy$, denoted $\abs{z}$, is the length of the vector $(x,y)$, or equivalently its distance from the origin; namely
\[\abs{z} \coloneqq \sqrt{(\Re z)^2 + (\Im z)^2} = \sqrt{x^2 + y^2} = \norm{(x,y)}\]
Notice that this extends the usual absolute value of real numbers, as the modulus of a real number is its absolute value.\\[0.5em]
We can then immediately derive a useful inequality,
\[\abs{z}^2 = (\Re z)^2 + (\Im z)^2 \geq (\Re z)^2,\  (\Im z)^2,\]
giving us \[\Re z \leq \abs{\Re z} \leq \abs{z}\quad \text{and} \quad \Im z \leq \abs{\Im z} \leq \abs{z}.\]
\end{definition}

\medskip

\begin{definition}[Distance]
The \cdef{distance} between two complex numbers $z_1$ and $z_2$ is \[\abs{z_1 - z_2} = \norm{(x_1,y_1) - (x_2,y_2)} = \norm{(x_1 - x_2,y_1 - y_2)}\]
That is, it's the euclidean distance between the vectors representing these complex numbers. 
\end{definition}

\medskip

\begin{discussion}\label{firstdomain}
The absolute value can be used to define various important subsets of $\cc$.
\begin{itemize}[itemsep=1em]
\item[(1)]
\begin{itemize}[itemsep=1em]
\item[$\bullet$] The \emph{circle of radius $R>0$ centered at $z_0$} is the set
\[C_R(z_0) = \setp{z\in \cc}{\abs{z-z_0}=R}\]
\item[$\bullet$] The \emph{open disk (or ball) of radius $R>0$ centered at $z_0$} is the set
\[D_R(z_0) = \setp{z\in \cc}{\abs{z-z_0}< R}\]\\[-1em]
\[\begin{tikzpicture}[scale=0.8]
    \draw[<->,thick] (-1,0)--(5,0);
	\draw[<->,thick] (0,-1)--(0,5);
    \draw[thick,firebrick](3,3) circle (2);
    \draw[](3,3)--(4.6,4.2);
    \fill (3,3) circle (2pt);
    \node[] at (3,2.5) {$z_0$};
    \node[] at (3,5.5) {\color{firebrick}$C_R(z_0)$};
    \node[] at (3.6,4) {$R$};
  \end{tikzpicture}
  \qquad \qquad \qquad
  \begin{tikzpicture}[scale=0.8]
    \draw[<->,thick] (-1,0)--(5,0);
	\draw[<->,thick] (0,-1)--(0,5);
	\filldraw[indigo,fill opacity=1/10,dashed](3,3) circle (2);
    \draw[](3,3)--(5,3);
    \fill (3,3) circle (2pt);
    \node[] at (2.5,3) {$z_0$};
    \node[] at (3,1.75) {\color{indigo}$D_R(z_0)$};
    \node[] at (4,3.4) {$R$};
  \end{tikzpicture}\]
\item[$\bullet$] The \emph{closed disk (or ball) of radius $R>0$ centered at $z_0$} is the set
\begin{align*}
\overline{D}_R(z_0) &= \setp{z\in \cc}{\abs{z-z_0} \leq R} = D_R(z_0) \cup C_R(z_0).
\end{align*}
\end{itemize}
\item[(2)] The \emph{(open) annulus of inner radius $r>0$ and outer radius $R>0$ centered at $z_0$} is the set
\[A_{r,R}(z_0) = \setp{z\in \cc}{r < \abs{z-z_0}<R}\]\\[-1em]
\[\begin{tikzpicture}[scale=0.8]
    \draw[<->,thick] (-1,0)--(5.5,0);
	\draw[<->,thick] (0,-1)--(0,5.5);
	\filldraw[teal,fill opacity=1/10,dashed](3,3) circle (2.5);
	\fill[white](3,3) circle (1.5);
    \draw[teal,dashed](3,3) circle (1.5);
    \draw[](3,3)--(5.5,3);
    \draw[](3,3)--(3.9,1.8);
    \fill (3,3) circle (2pt);
    \node[] at (3,3.5) {$z_0$};
    \node[] at (6.75,3) {\color{teal}$A_{r,R}(z_0)$};
    \node[] at (5,2.6) {$R$};
    \node[] at (3.2,2.2) {$r$};
  \end{tikzpicture}\]
\end{itemize}
\end{discussion}

%\bigskip

\begin{proposition}[Triangle Inequalities]\label{triangleineq}
For all $z_1,z_2 \in \cc$, the following inequalities hold.
\begin{itemize}
\item[(1)] $\abs{z_1 + z_2} \leq \abs{z_1} + \abs{z_2}$.
\item[(2)] $\abs{z_1 \pm z_2} \geq \abs{\abs{z_1} - \abs{z_2}}$. We sometimes refer to this inequality as the \cdef{reverse\ triangle\ inequality}.
\end{itemize}
\end{proposition}
\begin{proof}
\[\begin{tikzpicture}[scale=0.65]
    \draw[<->,thick] (-1,0)--(5.5,0);
	\draw[<->,thick] (0,-1)--(0,5.5);
    \fill[newblue] (5.5,5) circle (2pt) node[above]{\quad $z_1 + z_2$};
    \fill (0.5,3.5) circle (2pt) node[above]{$z_1$};
    \draw[->,>=stealth,thick] (0,0) -- (0.5,3.5);
    \draw[->,>=stealth,thick] (0,0) -- (5,1.5) node [pos=0.7, below,sloped] {$\abs{z_2}$};
	\draw[thick,dashed,newblue] (0.5,3.5)--(5.5,5);
	\draw[thick,newblue] (5,1.5)--(5.5,5) node [midway, right] {$\abs{z_1}$};
    \fill (5,1.5) circle (2pt) node[right]{$z_2$};
    \draw[->,>=stealth,thick,newblue] (0,0) -- (5.5,5) node [midway, above,sloped] {$\abs{z_1 + z_2}$};
  \end{tikzpicture}\]
\begin{itemize}
\item[(1)] A standard fact about triangles.
\item[(2)] We first assume that $\abs{z_1} \geq \abs{z_2}$. Then, $\abs{\abs{z_1} - \abs{z_2}} = \abs{z_1} - \abs{z_2}$. Now, note that
\begin{align*}
\abs{z_1} - \abs{z_2} &= \abs{z_1 \pm z_2 \mp z_2} - \abs{z_2}\\[0.5em]
 &\leq \abs{z_1 \pm z_2} + \abs{\mp z_2} - \abs{z_2},\ \text{triangle inequality}\\[0.5em]
 &= \abs{z_1 \pm z_2} + \abs{z_2} - \abs{z_2}\\[0.5em]
 &= \abs{z_1 \pm z_2}
\end{align*}
If we instead assume $\abs{z_2} \geq \abs{z_1}$, then we do the same computation with the roles of $z_1$ and $z_2$ switched.
\end{itemize}
\vspace*{-\baselineskip}
\end{proof}

\medskip


\begin{proposition}[Modulus is Multiplicative]\label{normmult}
\lecmargin{3}
For all $z,w \in \cc$ and positive integers $n$, 
\begin{multicols}{2}
\begin{itemize}
\item[(1)] $\abs{zw} = \abs{z}\abs{w}$.
\item[(2)] $\abs{z^n} = \abs{z}^n$.
\end{itemize}
\end{multicols}
\end{proposition}
\begin{proof}\hfill
\begin{itemize}
\item[(1)] Left as Problem \ref{prob 2.1a}. One proves these directly by showing that the left hand side matches the right hand side.
\item[(2)] The proof of this is by induction. $n = 1$ is a tautology, and $n = 2$ is (1) in the case $w = z$. Assume the statement is true for $n = k$, that is $\abs{z^k} = \abs{z}^k$. Then, for $n = k + 1$
\begin{align*}
|z^{k+1}| = |z^k\cdot z| &= |z^k|\abs{z},\quad \text{using (1)}\\[0.5em]
&= \abs{z}^k\abs{z},\quad \text{using the induction hypothesis}\\[0.5em]
&= \abs{z}^{k+1}
\end{align*}
Therefore we have the result by the principle of mathematical induction.
\end{itemize}
\vspace*{-\baselineskip}
\end{proof}

\medskip

\begin{definition}[Complex Conjugation]
Given a complex number $z = x + iy$, its \cdef{(complex)\ conjugate}, denoted $\overline{z}$, is
\[\overline{z} \coloneqq x - iy\]
Geometrically, $\overline{z}$ is the reflection of $z$ about the real axis. 
\[\begin{tikzpicture}[scale=0.75]
    \draw[<->,thick] (-1,0)--(5.5,0);
	\draw[<->,thick] (0,-2.75)--(0,2.75);
    \fill (3.5,2) circle (2pt) node[above right]{\ $z = x+iy$};
    \draw[->,>=stealth,thick] (0,0) -- (3.5,2);
    \fill (3.5,-2) circle (2pt) node[below right]{\ $\overline{z} = x-iy$};
    \draw[->,>=stealth,thick] (0,0) -- (3.5,-2);
    \draw[-,dashed] (3.5,2) -- (3.5,-2);
    \fill (3.5,0) circle (2pt) node[below right]{$x$};
    \fill (0,2) circle (2pt) node[left]{$y$};
    \fill (0,-2) circle (2pt) node[left]{$-y$};
  \end{tikzpicture}\]
\end{definition}

\medskip

\begin{proposition}[Properties of Conjugation]\label{conjprop}
For all pairs $z,w \in \cc$, we have
\begin{itemize}
\item[(1)] $\overline{\overline{z}} = z$
\item[(2)] $\abs{\overline{z}} = \abs{z}$
\item[(3)] $\overline{z + w} = \overline{z} + \overline{w}$
\item[(4)] $\overline{zw} = \overline{z}\ \overline{w}$
\item[(5)] $z\overline{z} = \abs{z}^2$
\item[(6)] $\Re z = \dfrac{z + \overline{z}}{2}$ and $\Im z = \dfrac{z - \overline{z}}{2i}$
\item[(7)] $z \in \rr$ if and only if $z = \overline{z}$
\end{itemize}
\end{proposition}
\begin{proof}
(1) -- (3) is clear geometrically. (4), (6) and (7) are left as Problem \ref{prob 2.1}, (7) can be proved using (6) and can also be deduced geometrically. One proves these directly by showing that the left hand side matches the right hand side.
\begin{itemize}
\item[(5)] Let $z = x + iy$, then
\begin{align*}
z\overline{z} &= (x + iy)(x - iy)\\[0.5em]
&= x^2 - ixy + iyx - i^2y^2\\[0.5em]
&= x^2 + y^2 + i(yx - xy) = x^2 + y^2 = \abs{z}^2\\[-1em]
\end{align*}
\end{itemize}
\vspace*{-\baselineskip}
\end{proof}

\medskip

\begin{discussion}
Proposition \ref{conjprop} (5) gives us a nice formula for $z^{-1}$ for $z\in \cc^*$. For such a $z$, we have $z\overline{z} = \abs{z}^2$, which gives us
\[z^{-1} = z^{-1}\cdot \frac{z\overline{z}}{\abs{z}^2} = \frac{\overline{z}}{\abs{z}^2}\]
This tells us that $z^{-1}$ is just a scaled $\overline{z}$, which means, geometrically speaking, $z^{-1}$ lies on the line passing through the origin and $\overline{z}$.
\end{discussion}

\medskip

\lecmargin{4}
Recall that every non-zero point $(x,y) \in \rr^2$ can be re-written in polar coordinates $(r,\theta)$ as
\[x = r\cos\theta \quad \text{and} \quad y = r\sin\theta\]
This suggests the following definition.
\begin{definition}[Polar Form]
If $(r,\theta)$ are polar coordinates for a non-zero $(x,y)$, then the \cdef{polar\ form} of a non-zero complex number $z = x + iy$ is
\[z = r(\cos\theta + i\sin\theta)\]
We sometimes abbreviate $\cos\theta + i\sin\theta$ as $\cis\theta$, so $z = r\cis\theta$.\\
\\
Evidently, $(r,\theta)$ are related to $(x,y)$ by the equations
\[\abs{z} = r \quad \text{and} \quad \cos\theta = \frac{x}{r} = \frac{x}{\sqrt{x^2 + y^2}},\ \sin\theta = \frac{y}{r} = \frac{y}{\sqrt{x^2 + y^2}},\ \text{so } \tan\theta = \frac{y}{x}\]
We have to be careful and take into account which quadrant $(x,y)$ belongs to, if we think of $\theta$ with respect to its formulation using $\tan$.
\[\begin{tikzpicture}[scale=0.75]
    \draw[<->,thick] (-1,0)--(5,0);
	\draw[<->,thick] (0,-1)--(0,5);
	\fill (4,3.5) circle (2pt) node[above]{\quad $z$};
    \draw[->,>=stealth,thick] (0,0) -- (4,3.5);
    \draw[|<->|,>=stealth,thick,firebrick] (-0.2478,0.2832) -- (3.752,3.783) node [fill=white, midway, sloped] {$r$};
    \draw
    (4,3.5) coordinate (a)
    -- (0,0) coordinate (b)
    -- (0.5,0) coordinate (c)
    pic["$\color{firebrick}\theta$", ->,>=stealth,thick, draw=firebrick, angle eccentricity=1.3, angle radius=1cm]
    {angle=c--b--a};
  \end{tikzpicture}\]
Since $\sin$ and $\cos$ are periodic functions, $\theta$ is not unique (you can replace $\theta$ with $\theta + 2\pi$). Each possible value of $\theta$ is called an \cdef{argument\ of} {\color{darkred}$z$}, and the set of all such $\theta$ is denoted as $\arg z$. That is, if $\theta_0$ is one solution of $\tan\theta = y/x$, then
\[\arg z = \setp{\theta_0 + 2k\pi}{k \in \zz}\]
The polar form, specifically $\theta$ is unique, as soon as we specify bounds on $\theta$. The unique argument in the interval $(-\pi,\pi]$ is called the \cdef{principal\ argument} denoted $\parg z$. Precisely speaking,
\begin{definition}\label{princ-arg}
For $z = x + iy$, we have
\[\parg z = \begin{cases}\arctan(y/x) & \text{if $x > 0$\quad \emph{\small(quadrants I \& IV)}}\\[0.5em]
\arctan(y/x) + \pi & \text{if $x < 0$ and $y > 0$\quad \emph{\small(quadrant II)}}\\[0.5em]
\arctan(y/x) - \pi & \text{if $x < 0$ and $y < 0$\quad \emph{\small(quadrant III)}}
 \end{cases}\]
\end{definition}
Notice that we can then write
\[\arg z = \setp{\parg z + 2k\pi}{k \in \zz}\]
\end{definition}

\medskip

\begin{definition}[Euler's Formula]\label{eulerform}
$e^{i\theta} \coloneqq \cis\theta = \cos\theta + i\sin\theta$. Therefore $\abs{e^{i\theta}} = 1$.
\end{definition}
\begin{proof}[Remark on Definition \ref{eulerform}]\renewcommand{\qedsymbol}{}
This is for now a stopgap, defining $e^{i\theta}$ in this way. In a few weeks, we'll see that this is truly an equality of holomorphic functions. Euler deduced this by looking at the Taylor series expansion of these functions. We haven't built or discussed enough machinery to give this reasoning a solid foundation yet. 
\end{proof}

\medskip

Using Euler's formula, one can write the polar form of a non-zero complex number, even more succinctly in its \cdef{exponential\ form}
\[z = re^{i\theta}\]
\begin{example}\hfill
\begin{itemize}[itemsep=1em]
\item[(1)] Exponential form of $1 + i$, 
\[\abs{1 + i} = \sqrt{1^2 + 1^2} = \sqrt{2}\quad \text{and} \quad \parg z = \arctan(1) = \frac{\pi}{4}\]
So, $1 + i = \sqrt{2}e^{i\pi/4}$.
\[\begin{tikzpicture}[scale=1.5]
    \draw[<-,thick] (-1.5,0)--(-1,0);
	\draw[->,thick] (0,1)--(0,1.5);
	\draw[->,thick] (1,0)--(1.5,0);
	\draw[<-,thick] (0,-1.5)--(0,-1);
	\fill[teal] (1,1) circle (0.8pt) node[above right]{\color{teal}$1+i$};
    \node (a) at (1,1) {};
    \node (b) at (0,0) {};
    \node (c) at (0.5,0) {};
    \draw pic["{\footnotesize$\color{teal}\theta = \dfrac{\pi}{4}$}", ->,>=stealth,thick, draw=teal, angle eccentricity=1.7, angle radius=1cm] {angle=c--b--a};
  \draw[->,>=stealth,teal,thick] (0,0) -- (1,1);
  \draw[->,>=stealth,firebrick,thick] (0,0) -- (1,0);
  \fill[firebrick] (1,0) circle (0.8pt) node[below]{\color{firebrick}$1$};
  \draw[->,>=stealth,firebrick,thick] (0,0) -- (0,1);
  \fill[firebrick] (0,1) circle (0.8pt) node[right]{\color{firebrick}$i$};
  \draw[->,>=stealth,firebrick,thick] (0,0) -- (-1,0);
  \fill[firebrick] (-1,0) circle (0.8pt) node[above]{\color{firebrick}$-1$};
  \draw[->,>=stealth,firebrick,thick] (0,0) -- (0,-1);
  \fill[firebrick] (0,-1) circle (0.8pt) node[left]{\color{firebrick}$-i$};
    \end{tikzpicture}\]
\item[(2)] Note that
\[1 = e^{i0} = e^{i2n\pi}\ \text{for any $n \in \zz$},\qquad i = e^{i\pi/2},\qquad -1 = e^{i\pi} = e^{i(2n+1)\pi}\ \text{for any $n \in \zz$}\]
One could write $-i = e^{i3\pi/2}$ but $3\pi/2 \neq \parg(-i)$; instead we should write $-i = e^{-i\pi/2}$.

\item[(3)] The circle $C_R(z_0)$ has a nice parametrisation 
\[C_R(z_0) = \{z = z_0 + Re^{i\theta}\ :\ 0 \leq \theta < 2\pi\}\]\\[-1.5em]
\[\begin{tikzpicture}[scale=0.75]
    \draw[<->,thick] (-1,0)--(5,0);
	\draw[<->,thick] (0,-1)--(0,5);
	\draw[->,>=stealth,thick] (0,0) -- (3,3);
    \draw[thick,indigo](3,3) circle (2);
    \draw[thick](3,3)--(5,3) node[midway, below]{$R$};
    \fill (3,3) circle (2pt);
    \node[left] at (3,3) {$z_0$};
    \draw[thick](3,3)--(2,4.732);
    \fill[indigo] (2,4.732) circle (2pt) node[above left]{$z$};
    \node (a) at (2,4.732) {};
    \node (b) at (3,3) {};
    \node (c) at (5,3) {};
    \draw pic["{\footnotesize$\theta$}", ->,>=stealth,thick,draw, angle eccentricity=1.5, angle radius=0.5cm] {angle=c--b--a};
    \node[] at (5,1) {\color{indigo}$C_R(z_0)$};
  \end{tikzpicture}\]
\end{itemize}
\vspace*{-\baselineskip}
\end{example}

\medskip

\begin{example}[in-class]
Write the exponential form of $z = 1 - i$.
\end{example}
\begin{proof}[Answer]
As a point on the plane, since $\Re z > 0$ and $\Im z < 0$, the complex number $z = 1 - i$ lies in the fourth quadrant. Thus, 
\begin{align*}
r = \abs{z} &= \sqrt{(1)^2 + (-1)^2} = \sqrt{2}\\[0.5em]
\parg z &= \arctan(-1) = -\frac{\pi}{4}
\end{align*}
Thus, $1 - i = \sqrt{2}e^{-i \pi/4}$. 
\end{proof}

\medskip

\begin{proposition}[Properties of Exponential Form]\label{propeuler}
Let $z = re^{i\theta}$ and $w = se^{i\phi}$ be non-zero complex numbers. Then
\begin{itemize}
\item[(1)] $zw = rs\  e^{i(\theta + \phi)}$
\item[(2)] $z^{-1} = (1/r) e^{-i\theta}$
\item[(3)] $z^n = r^n e^{in\theta}$, for any $n \in \zz$
\item[(4)] $\overline{z} = re^{-i\theta}$
\item[(5)] $z/w = (r/s)e^{i(\theta - \phi)}$
\end{itemize}
\end{proposition}
\begin{proof}\hfill
\begin{itemize}
\item[(1)] Note that
\begin{align*}
zw = (re^{i\theta})(se^{i\phi})&= rs(\cos\theta + i\sin\theta)(\cos\phi + i\sin\phi)\\[0.5em]
&= rs((\cos\theta\cos\phi - \sin\theta\sin\phi) + i(\cos\theta\sin\phi + \sin\theta\cos\phi))\\[0.5em]
&= rs(\cos(\theta + \phi) + i\sin(\theta + \phi))\\[0.5em]
&= rs\  e^{i(\theta + \phi)}
\end{align*}
\item[(2)] It suffices to show that $(re^{i\theta})((1/r) e^{-i\theta}) = 1$, for which we use (1).
\item[(3)] We first prove this result for $n \geq 0$, the result is clear for $n = 0$ and $n = 1$. Assume the result is true for $n = k$, that is $z^k = r^k e^{ik\theta}$. Then, for $n = k+1$
\begin{align*}
z^{k+1} &= z^kz\\[0.5em]
&= (r^k e^{ik\theta})(re^{i\theta})\ \text{using the induction hypothesis}\\[0.5em]
&= r^{k+1} e^{ik\theta+\theta}\ \text{by (1)}\\[0.5em]
&= r^{k+1} e^{i(k+1)\theta}
\end{align*}
Therefore we have the result by the principle of mathematical induction.\\[0.5em]
Suppose $n<0$ instead, then write $n = -m$ for a positive $m>0$. Now, we can apply the first case to $z^n \coloneqq (z^{-1})^m$ to get our result.
\item[(4)] Using $z\overline{z} = \abs{z}^2 = r^2$, we get that $\overline{z} = r^2z^{-1}$, and the result follows from (2).
\item[(5)] Recall $z/w = zw^{-1}$, and the result follows from (2) and (1).
\end{itemize}
\vspace*{-\baselineskip}
\end{proof}

\medskip

\begin{discussion}
Proposition \ref{propeuler} (1) gives us a nice geometric interpretation of complex multiplication. If $z = re^{i\theta}$ and $w = se^{i\phi}$, then $zw = rs\  e^{i(\theta + \phi)}$. This can be interpreted as saying that $zw$ is obtained from $w$ by scaling $w$ by $\abs{z} = r$ and rotating $w$ by an angle of $\parg z$ (or vice versa).
\[\begin{tikzpicture}
    \draw[<->,thick] (-5,0)--(5,0);
	\draw[<->,thick] (0,-1)--(0,5);
	\fill[firebrick] (2.5,1.5) circle (2pt) node[above right]{$z$};
    \draw[->,>=stealth,thick,firebrick] (0,0) -- (2.5,1.5);
    \node (a) at (2.5,1.5) {};
    \node (b) at (0,0) {};
    \node (c) at (0.2,0) {};
    \draw pic["$\color{firebrick}\theta$", ->,>=stealth,thick, draw=firebrick, angle eccentricity=1.3, angle radius=1cm] {angle=c--b--a};
    
	\fill[indigo] (0.75,3) circle (2pt) node[above]{$w$};
    \draw[->,>=stealth,thick,indigo] (0,0) -- (0.75,3);
    \node (a) at (0.75,3) {};
    \node (b) at (0,0) {};
    \node (c) at (0.2,0) {};
    \draw pic["$\color{indigo}\phi$",left, ->,>=stealth,thick, draw=indigo, angle eccentricity=2, angle radius=0.5cm] {angle=c--b--a};

	\fill[forest] (-1.5,4) circle (2pt) node[above]{$zw$};
    \draw[->,>=stealth,thick,forest] (0,0) -- (-1.5,4);
    \node (a) at (-1.5,4) {};
    \node (b) at (0,0) {};
    \node (c) at (0.2,0) {};
    \draw pic["\quad$\color{forest}\theta + \phi$", ->,>=stealth,thick, draw=forest, angle eccentricity=1.2, angle radius=1.6cm] {angle=c--b--a};
%    \draw[|<->|,>=stealth,thick] (-2.158,3.754) -- (-0.6575,-0.247) node [fill=white, midway, sloped] {$\abs{z}\abs{w}$};
    \draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=4pt},yshift=0pt,thick,forest]
(-1.5,4) -- (0,0) node [black,midway,left,xshift=-0.4cm,yshift=-0.2cm] {$\color{forest}rs\ $};
  \end{tikzpicture}\]

\medskip

\begin{example}
Let's use Proposition \ref{propeuler} to compute $(1+i)^{2023}$, then
\begin{align*}
(1 + i)^{2023} &= (\sqrt{2} e^{i\pi/4})^{2023}\\[0.5em]
 &= (\sqrt{2})^{2023}(e^{i\pi/4})^{2023}\\[0.5em]
 &= (\sqrt{2})^{2022}\sqrt{2}(e^{i\pi/4})^{2024}e^{-i\pi/4}\\[0.5em]
 &= 2^{1011}\sqrt{2}(e^{i506\pi})e^{-i\pi/4}\\[0.5em]
&= 2^{1011}\sqrt{2}e^{-i\pi/4}\\[0.5em]
&= 2^{1011}(1-i)
\end{align*}
\end{example}

\medskip

\begin{example}
Compute $(1+i\sqrt{3})^{101}$.
\end{example}
\begin{proof}[Answer]
We will first compute the exponential form of our complex number. Note that \[|1 + i\sqrt{3}| = \sqrt{1^2 + (\sqrt{3})^2} = \sqrt{4} = 2,\] and since $1 + i\sqrt{3}$ lies in the first quadrant of the complex plane
\[\parg z = \arctan(\sqrt{3}) = \frac{\pi}{3}\]
Therefore
\[1+i\sqrt{3} = 2e^{i\pi/3}\]
and so
\begin{align*}
(1+i\sqrt{3})^{101} &= (2e^{i\pi/3})^{101}\\[0.5em]
 &= (2e^{i\pi/3})^{99}(2e^{i\pi/3})^{2}\\[0.5em]
 &= 2^{99}e^{i33\pi}(1+i\sqrt{3})^{2}\\[0.5em]
 &= -2^{99}(1-3 +2i\sqrt{3}),\quad \text{since $33$ is odd}\\[0.5em]
 &= -2^{99}(-2 +2i\sqrt{3})\\[0.5em]
 &= 2^{100}(1- i\sqrt{3})
\end{align*}
\end{proof}

\medskip

\lecmargin{5}
A few more interesting consequences of Proposition \ref{propeuler} are
\begin{itemize}
\item[(1)] The \emph{unit circle} \[S^1 = \setp{z\in \cc}{\abs{z} = 1} = \{e^{i\theta}\ :\ \theta \in \rr\}\] is closed under multiplication. It's in fact an abelian group, usually denoted $U(1)$.
\item[(2)] \emph{De Moivre's Theorem}. From Proposition \ref{propeuler} (4) applied to $z = e^{i\theta}$ we get
\[(\cos\theta + i\sin\theta)^n = (\cos n\theta + i\sin n\theta)\]
\end{itemize}
\vspace*{-\baselineskip}
\end{discussion}

\medskip

\begin{proposition}[Arguments of Products]\label{prodarg}
Let $z,w$ be non-zero complex numbers, then
\begin{itemize}
\item[(1)] $\arg (zw) = \arg z + \arg w$
\item[(2)] $\arg w^{-1} = -\arg w$
\end{itemize}
\emph{Note that this is \emph{not} saying $\parg(zw) = \parg z + \parg w$, this is actually not true, we're claiming an equality of sets. (1) and (2) together give us $\arg (z/w) = \arg z - \arg w$.}
\end{proposition}
\begin{proof}\hfill
\begin{itemize}
\item[(1)] Consider $\theta \in \arg z$ and $\phi \in \arg w$, so $z = re^{i\theta}$ and $w = se^{i\phi}$. By Proposition \ref{propeuler} (1), we have $zw = rs\ e^{i(\theta + \phi)}$ and therefore $\theta + \phi \in \arg(zw)$. Hence $\arg z + \arg w \subseteq \arg(z + w)$.\\[0.5em]
Consider $\psi \in \arg(z+w)$, and some $\theta \in \arg z$ then we claim that $\psi - \theta \in \arg w$. We have $rs\ e^{i\psi} = zw = re^{i\theta}w$, then by Proposition \ref{propeuler} (5), we get $w = sr^{i(\psi - \theta)}$. Hence $\psi - \theta \in \arg w$, and since $\psi = \theta + (\psi - \theta) \in \arg z + \arg w$, we have $\arg(z+w) \subseteq \arg z + \arg w$.\\[0.5em]
Therefore $\arg (zw) = \arg z + \arg w$.
\item[(2)] Consider $\theta \in \arg z$, so $z = re^{i\theta}$. By Proposition \ref{propeuler} (2), we have $z^{-1} = (1/r)e^{i(-\theta)}$ and therefore $-\theta \in \arg w^{-1}$. Hence $-\arg w \subseteq \arg w^{-1}$.\\[0.5em]
Note that $w = (w^{-1})^{-1}$, applying the above result to $w^{-1}$ gets us $-\arg w^{-1} \subseteq \arg (w^{-1})^{-1} = \arg w$ and so $\arg w^{-1} \subseteq - \arg w$.\\[0.5em]
Therefore $\arg w^{-1} = -\arg w$.
\end{itemize}
\vspace*{-\baselineskip}
\end{proof}

\medskip

\begin{remark}
For a complex number, $\arg z$ is a set of all possible $\theta$'s such that we can write $z = \abs{z}e^{i\theta}$, as you know. Therefore, we will abuse notation by sometimes calling any $\theta \in \arg z$ as an argument of $z$, and sometimes also writing $z = \abs{z}e^{i\arg z}$. That is, we are not, or are careless about, distinguishing the set $\arg z$ and its element when we can be agnostic about the choice of $\theta$; for example, the polar form of a complex number. It will be clear when we choose to care about out choice, it will be evident because we'll be then forcing $\theta$ to lie in an interval of length $2\pi$; for example, the principal argument $-\pi < \parg z \leq \pi$.
\end{remark}

\medskip

\begin{example}\hfill
\begin{itemize}
\item[(1)] The principal argument of $z = (\sqrt{3} - i)^6$. We first note that $\parg(\sqrt{3} - i) = -\pi/6$. By Proposition \ref{prodarg} (1), applied inductively, we have
\begin{align*}
\arg (\sqrt{3} - i)^6 &= \underbrace{\arg(\sqrt{3} - i) + \cdots + \arg(\sqrt{3} - i)}_{\text{$6$ times}} = \setp{-\pi + 2k\pi}{k \in \zz}
\end{align*}
Then $\parg(\sqrt{3} - i)^6$ is the element in the set above in the interval $(-\pi,\pi]$ which is $\pi$.
\item[(2)] As mentioned previously, we can't just replace $\arg$ with $\parg$ in the statement of Proposition \ref{prodarg} (1). Here's a simple example: let $z = w = -1$, then $\parg z = \parg w = \pi$ and $\parg zw = \parg 1 = 0$ but $0 \neq 2\pi = \parg z + \parg w$.
\item[(3)] Note that $\arg z + \arg z \neq 2\arg z$.
\end{itemize}
\vspace*{-\baselineskip}
\end{example}

\bigskip

\subsection{Roots of Complex Numbers}
%\begin{mdframed}
%\begin{center}
%{\Large Roots of Complex Numbers}
%\end{center}
%\end{mdframed}

\begin{lemma}\label{polareq}
Two non-zero complex numbers $z,w$ are equal if and only if $\abs{z} = \abs{w}$ and $\arg z = \arg w$.
\end{lemma}
\begin{proof}
If $\abs{z} = \abs{w}$ and $\arg z = \arg w$, then clearly $z = w$.\\[0.5em]
Suppose $z = w$, then we immediately get $\abs{z} = \abs{w}$. Consider $\theta \in \arg z$ and $\phi \in \arg w$, then we get $e^{i\theta} = e^{i\phi}$ which is equivalent to saying $\cos(\theta - \phi) + i\sin(\theta - \phi) = e^{i(\theta - \phi)} = 1$. This gives us
\[\sin (\theta - \phi) = 0.\]
The solution to this is $\theta - \phi = 2k\pi$ for some $k\in \zz$. This gives us $\arg z = \arg w$.
\end{proof}

\medskip

\begin{definition}[Roots]
\lecmargin{6}
Let $\alpha$ be a non-zero complex number. An \emph{$n^{\text{th}}$ root of $\alpha$} is a solution to the polynomial equation $z^n - \alpha = 0$.\\
\\
The set of all $n^{\text{th}}$ roots of $\alpha$ is denoted by $\alpha^{1/n}$, we reserve the symbol $\sqrt[n]{\ \cdot\ }$ for the unique positive $n^{\text{th}}$ root of a positive real number.
\end{definition}

\medskip

\begin{proposition}[Distinct Roots]\label{distroot}
There are precisely $n$ distinct $n^{\text{th}}$ roots of $\alpha$, namely
\[\beta_k = \sqrt[n]{\abs{\alpha}}\ e^{i\left(\frac{\parg \alpha}{n} + \frac{2k\pi}{n}\right)},\quad k = 0,\ldots,n-1\]
\end{proposition}
\begin{proof}
Let $z = re^{i\theta}$ and $\alpha = \abs{\alpha}e^{i\parg\alpha}$, we solve
\[r^ne^{in\theta} = z^n = \alpha = \abs{\alpha}e^{i\parg\alpha}.\]
By Lemma \ref{polareq}, this equality is true if and only if $r^n = \abs{\alpha}$ and $n\theta = \parg\alpha + 2k\pi$ for some $k \in \zz$. Therefore
\[z = \sqrt[n]{\abs{\alpha}}\ e^{i\left(\frac{\parg \alpha}{n} + \frac{2k\pi}{n}\right)},\quad k \in \zz\]
We obtain distinct $n$ complex numbers for $k = 0,\ldots,n-1$ since they have distinct arguments, and they necessarily give us the $n$ distinct $n^{\text{th}}$ roots of $\alpha$.
\end{proof}

\medskip

\begin{discussion}
With the notation of Proposition \ref{distroot}, the {\color{darkred}$n^{\text{th}}$} \cdef{principal\ root\ of} {\color{darkred}$\alpha$} is
\[\beta_0 = \sqrt[n]{\abs{\alpha}}\ e^{i\frac{\parg \alpha}{n}}\]
If we introduce the notation $\zeta_n = e^{\frac{2\pi i}{n}}$, then
\[\zeta_n^k = e^{\frac{2k\pi i}{n}}\]
According to the proposition, the complex numbers
\[1,\ \zeta_n,\ \zeta_n^2,\ldots,\zeta_n^{n-1}\]
are the distinct solutions to $z^n - 1 = 0$, the {\color{darkred}$n^{\text{th}}$} \cdef{roots\ of\ unity}, making $\zeta_n$ the \cdef{primitive} \emph{$n^{\text{th}}$ root of unity} as it generates all $n^{\text{th}}$ other roots of unity.\\
\\
Then we can write the roots of $\alpha$ in terms of the principal root and the primitive root of unity
\begin{align*}
\beta_k &= \sqrt[n]{\abs{\alpha}}\ e^{i\left(\frac{\parg \alpha}{n} + \frac{2k\pi}{n}\right)}\\[0.5em]
 &= \sqrt[n]{\abs{\alpha}}\ e^{i\frac{\parg \alpha}{n}}e^{\frac{2k\pi i}{n}}  = \beta_0\zeta_n^k
\end{align*}
That is, $\beta_k$'s all lie on the circle of radius $\sqrt[n]{\abs{\alpha}}$ centered at the origin, and all of them are obtained by rotating $\beta_0$ by an angle of $2k\pi/n$. That is, they all lie on the vertices of an inscribed regular $n$-gon.
\[\begin{tikzpicture}
    \draw[<->,thick] (-2.5,0)--(2.5,0);
	\draw[<->,thick] (0,-3)--(0,3);
    \draw[thick](0,0) circle (2);
    \draw[->,>=stealth,thick,firebrick](0,0)--(-0.41,1.958);
    \draw[->,>=stealth,thick,firebrick](0,0)--(-1.4,1.428);
    \fill[firebrick] (-0.41,1.958) circle (2pt) node[above]{\color{firebrick}$\beta_{k-1}$};
    \fill[firebrick] (-1.4,1.428) circle (2pt) node[above left]{\color{firebrick}$\beta_{k}$};
    \node (a) at (-1.4,1.428) {};
    \node (b) at (0,0) {};
    \node (c) at (-0.41,1.958) {};
    \draw[->,>=stealth,thick,firebrick](0,0)--(1.73,1.004) node[midway,above,sloped]{\color{firebrick}$\sqrt[n]{\abs{\alpha}}$};
    \fill[firebrick] (1.73,1.004) circle (2pt) node[above right]{\color{firebrick}$\beta_0$};
    \draw pic["{\scriptsize$\color{firebrick}\ \dfrac{2\pi}{n}$}", ->,>=stealth,thick,draw=firebrick, angle eccentricity=1.4, angle radius=1cm] {angle=c--b--a};
  \end{tikzpicture}\]
\end{discussion}

\medskip

\begin{example}\hfill
\begin{itemize}
\item[(1)] We compute explicitly the $4^{\text{th}}$ roots of $\alpha = -16$. As a negative real number, $\parg(-16) = \pi$, so
\begin{align*}
\beta_k &= \sqrt[4]{16}e^{i\left(\frac{\pi}{4}+\frac{2k\pi}{4}\right)} = 2\ e^{i\frac{\pi}{4}}e^{\frac{ki\pi}{2}}\\[0.5em]
&= 2\ e^{i\frac{\pi}{4}}\left(e^{\frac{i\pi}{2}}\right)^k\\[0.5em]
&= 2\ \left(\cos\frac{\pi}{4} + i\sin\frac{\pi}{4}\right)\left(\cos\frac{\pi}{2} + i\sin\frac{\pi}{2}\right)^k = 2\ \left(\frac{1}{\sqrt{2}} + i\frac{1}{\sqrt{2}}\right)i^k = \sqrt{2}(1 + i)i^k
\end{align*}
Therefore
\[\begin{tikzpicture}
    \draw[<->,thick] (-2.5,0)--(2.5,0);
	\draw[<->,thick] (0,-2.5)--(0,2.5);
    \draw[thick](0,0) circle (2);
    \draw[->,>=stealth,thick,indigo](0,0)--(1.414,1.414) node[above, midway, sloped]{\color{indigo}$2$};
    \draw[thick,indigo,dashed](1.414,1.414)--(1.414,-1.414);
    \draw[thick,indigo,dashed](1.414,-1.414)--(-1.414,-1.414);
    \draw[thick,indigo,dashed](-1.414,-1.414)--(-1.414,1.414);
    \draw[thick,indigo,dashed](-1.414,1.414)--(1.414,1.414);
    \fill[indigo] (1.414,1.414) circle (2pt) node[above right]{\color{indigo}$\sqrt{2}(1 + i)$};
    \fill[indigo] (1.414,-1.414) circle (2pt) node[below right]{\color{indigo}$\sqrt{2}(1 - i)$};
    \fill[indigo] (-1.414,1.414) circle (2pt) node[above left]{\color{indigo}$\sqrt{2}(-1 + i)$};
    \fill[indigo] (-1.414,-1.414) circle (2pt) node[below left]{\color{indigo}$\sqrt{2}(-1 - i)$};
    \node (a) at (1.414,1.414) {};
    \node (b) at (0,0) {};
    \node (c) at (2,0) {};
    \draw pic["{\scriptsize$\color{indigo}\ \dfrac{\pi}{4}$}", ->,>=stealth,thick,draw=indigo, angle eccentricity=1.25, angle radius=1cm] {angle=c--b--a};
  \end{tikzpicture}\]
\[\beta_0 = \sqrt{2}(1 + i),\quad \beta_1 = \sqrt{2}(-1 + i),\quad \beta_2 = \sqrt{2}(-1-i),\quad \beta_3 = \sqrt{2}(1-i)\]
\item[(2)] In the course of the previous example, we have computed the $4^{\text{th}}$ roots of unity, since they are
\[e^{\frac{2ki\pi}{4}} = e^{\frac{ki\pi}{2}},\quad k = 0,1,2,3\]
as $\parg 1 = 0$. Letting $\zeta_4 = e^{i\pi/2} = i$, the $4^{\text{th}}$ roots of unity are $\zeta_4^0,\zeta_4^1,\zeta_4^2,\zeta_4^3$, which are nothing but $\pm 1,\pm i$. Furthermore, note that $i$ is the primitive $4^{\text{th}}$ root of unity. 
\end{itemize}
\end{example}

\medskip

\begin{example}\label{cuberootofunity}
\lecmargin{7}
We compute the $3^{\text{rd}}$ roots of unity, also called the cube roots of unity where we denote $\omega = \zeta_3$, explicitly.
\[\begin{tikzpicture}[scale=0.8]
    \draw[<->,thick] (-3,0)--(3,0);
	\draw[<->,thick] (0,-3)--(0,3);
    \draw[thick](0,0) circle (2);
    \draw[->,>=stealth,thick,dirt](0,0)--(-1,1.732);
    \draw[thick,dirt,dashed](2,0)--(-1,1.732);
    \draw[thick,dirt,dashed](-1,1.732)--(-1,-1.732);
    \draw[thick,dirt,dashed](-1,-1.732)--(2,0);
    \fill[dirt] (-1,1.732) circle (2pt) node[above left]{\color{dirt}$\omega$};
    \fill[dirt] (-1,-1.732) circle (2pt) node[below left]{\color{dirt}$\omega^2$};
    \fill[dirt] (2,0) circle (2pt) node[below right]{\color{dirt}$1$};
    \node (a) at (-1,1.732) {};
    \node (b) at (0,0) {};
    \node (c) at (2,0) {};
    \draw pic["{\ \ \tiny$\color{dirt}\ \dfrac{2\pi}{3}$}", ->,>=stealth,thick,draw=dirt, angle eccentricity=1.6, angle radius=0.3cm] {angle=c--b--a};
  \end{tikzpicture}\]
Let the primitive root be $\omega = \zeta_3$, then the cube roots of unity are
\[1,\ \omega,\ \omega^2\]
where we have
\begin{align*}
\omega = e^{\frac{2\pi i}{3}} &= \left(\cos\frac{2\pi}{3} + i\sin\frac{2\pi}{3}\right) = -\frac{1}{2}+i\frac{\sqrt{3}}{2}\\[0.5em]
\omega^2 = e^{\frac{4\pi i}{3}} &= \left(\cos\frac{4\pi}{3} + i\sin\frac{4\pi}{3}\right) = -\frac{1}{2}-i\frac{\sqrt{3}}{2}
\end{align*}
\end{example}

\bigskip

\subsection{Basic Topology of $\cc$}
%\begin{mdframed}
%\begin{center}
%{\Large Basic Topology of $\cc$}
%\end{center}
%\end{mdframed}

Our purpose now is to define the kind of subsets of $\cc$ that are suitable for doing complex analysis, namely \emph{non-empty open connected sets}.
\begin{definition}[Open Disks or Neighbourhoods]
Let $\epsilon>0$. Recall the \cdef{open\ disk} (of radius $\epsilon$ centered at $z_0$) is the set
\[D_\epsilon(z_0) = \setp{z\in \cc}{\abs{z - z_0}<\epsilon}.\]
We also refer to such an open disk as an {\color{darkred}$\epsilon$-}\cdef{neighbourhood} or simply a \cdef{neighbourhood}.\\
\\
A \cdef{deleted} (or \cdef{punctured}) \cdef{open\ disk} (or \cdef{neighbourhood}) is a set of the form
\[D_\epsilon(z_0)\setminus\set{z_0} = \setp{z\in \cc}{0 < \abs{z - z_0}<\epsilon}.\]\\[-0.5em]
\[\begin{tikzpicture}[scale=0.75]
    \draw[<->,thick] (-1,0)--(5,0);
	\draw[<->,thick] (0,-1)--(0,5);
	\filldraw[teal,fill opacity=1/7,dashed](3,3) circle (2);
    \draw[](3,3)--(4.86,3.735) node[sloped,midway,above]{$\epsilon$};
    \fill (3,3) circle (2pt) node[below left]{$z_0$};
  \end{tikzpicture}
  \qquad\qquad\qquad
  \begin{tikzpicture}[scale=0.75]
    \draw[<->,thick] (-1,0)--(5,0);
	\draw[<->,thick] (0,-1)--(0,5);
	\filldraw[forest,fill opacity=1/7,dashed](3,3) circle (2);
    \draw[forest,dotted](3,3) circle (3pt);    
    \draw[](3,3)--(4.86,3.735) node[sloped,midway,above]{$\epsilon$};
    \fill[white] (3,3) circle (3pt) node[below left]{\color{black}$z_0$};
  \end{tikzpicture}\]
Points belonging to the same $\epsilon$-neighbourhood are considered "close" to each other, in the sense that they are within a distance of $2\epsilon$ from each other.
\end{definition}

\medskip

\begin{definition}[Various kinds of Points]
Consider a $S \subseteq \cc$. 
\begin{itemize}
\item A point $z \in S$ is an \cdef{interior\ point\ of} {\color{darkred}$S$} if there exists an $\epsilon > 0$ such that $D_\epsilon(z) \subseteq S$. 
\item A point $z \notin S$ is an \cdef{exterior\ point\ of} {\color{darkred}$S$} if there exists an $\epsilon > 0$ such that $D_\epsilon(z) \cap S = \emptyset$. 
\item A point $z \in \cc$ is a \cdef{boundary\ point\ of} {\color{darkred}$S$} if it's neither an interior nor an exterior point of $S$. Equivalently, if every neighbourhood of $z$ contains both a point in $S$ and not in $S$.
\item A point $z \in \cc$ is a \cdef{accumulation} (or \cdef{cluster}) \cdef{point\ of} {\color{darkred}$S$} if for every $\epsilon > 0$ we have \[D_\epsilon(z)\setminus\set{z} \cap S \neq \emptyset.\]
\item A point $z \in S$ is an \cdef{isolated\ point\ of} {\color{darkred}$S$} if there exists an $\epsilon > 0$ such that $D_\epsilon(z)\setminus\set{z} \cap S = \emptyset$. Isolated points are examples of boundary point
\end{itemize}
\[\begin{tikzpicture}[scale=1.1]
    \draw[<->,thick] (-2,0)--(5,0);
	\draw[<->,thick] (0,-2)--(0,5);
    \node[] at (4.75,2) {\color{dirt}$S$};
    
    \fill (3,4) circle (2pt) node[below]{\scriptsize$z_1$};
    \draw[](3,4)--(3.4,4.3) node[sloped,midway,xshift=-1pt,yshift=4pt]{\tiny$\epsilon_1$};
    \filldraw[indigo,fill opacity=1/10,dashed](3,4) circle (0.5);
    
    \fill (2.7,0.85) circle (2pt) node[below]{\scriptsize$z_2$};
    \draw[](2.7,0.85)--(3.45,0.85) node[sloped,midway,above,yshift=-1pt]{\tiny$\epsilon_2$};
    \filldraw[newblue,fill opacity=1/10,dashed](2.7,0.85) circle (0.75);
    
    \fill (-0.75,3.25) circle (2pt) node[below]{\scriptsize$z_3$};
    \draw[](-0.75,3.25)--(-1.75,3.25) node[sloped,midway,above,yshift=-1pt]{\tiny$\epsilon_3$};
    \filldraw[firebrick,fill opacity=1/10,dashed](-0.75,3.25) circle (1);
    
    \path[draw,use Hobby shortcut,closed=true,fill=dirt,fill opacity=1/10,dashed]
(-1,0) .. (3,0) .. (3,4) .. (1.5,2) .. (-1,0);
	\fill[dirt] (5,3) circle (2pt) node[below]{\scriptsize$z_4$};
\end{tikzpicture}\]
Here $z_1$ is a boundary point, $z_2$ an interior point, $z_3$ an exterior point, and $z_4$ is an isolated point (and a boundary point).
\end{definition}

%\medskip

\begin{remark}
The idea is that if we don't move too far from an interior point of $S$ then we remain in $S$; a similar idea holds for an exterior point. But at a boundary point we can make an arbitrarily small move and get to a point inside $S$, and we can also make an arbitrarily small move and get to a point outside $S$. An accumulation point is one where it has other points from $S$ within any arbitrarily small distance, i.e. points "accumulate" near it; an isolated point is the exact opposite.
\end{remark}

\medskip

\begin{definition}[Open and Closed Sets]
Consider a $S \subseteq \cc$.
\begin{itemize}
\item The \cdef{interior\ of} {\color{darkred}$S$} is the set of all interior points of $S$, denoted $S^\circ$.
\item $S$ is said to be \cdef{open} if $S = S^\circ$.
\item The \cdef{boundary\ of} {\color{darkred}$S$} is the set of all boundary points of $S$, denoted $\partial S$.
\item $S$ is said to be \cdef{closed} if $\partial S \subseteq S$. Equivalently, if its complement is open.
\item The \cdef{closure\ of} {\color{darkred}$S$} is the set $S \cup \partial S$, denoted $\overline{S}$.
\end{itemize}
\end{definition}

\medskip

\begin{example}\hfill
\begin{itemize}
\item[(1)] The open disks $D_R(z_0)$ are truly open sets, and the closed disks $\overline{D}_R(z_0)$ are truly closed sets.\\[0.5em]
The closure of the open disk $D_R(z_0)$ is $\overline{D}_R(z_0)$. The boundary of $D_R(z_0)$ is the circle $C_R(z_0)$.
\item[(2)] Consider the upper half-plane 
\[\mathbf{H} = \setp{z\in \cc}{\Im z>0},\]
then we have $\mathbf{H}^\circ = \mathbf{H}$. Since by definition $\mathbf{H}^\circ \subseteq \mathbf{H}$, it's enough to prove $\mathbf{H} \subseteq \mathbf{H}^\circ$. Consider any $z \in \mathbf{H}$, then $\Im z > 0$. Let $\epsilon = (\Im z)/2$, we claim that
\[D_\epsilon(z) \subseteq \mathbf{H}\]\\[-1em]
\[\begin{tikzpicture}[scale=0.9]
    \draw[<->,thick] (-5,0)--(5,0);
	\draw[<->,thick] (0,-2)--(0,5.5);
    \node[] at (5.5,2.25) {\color{dirt}$\mathbf{H}$};
	\draw[dashed,dirt] (-5,5)--(5,5);
    \fill[dirt,fill opacity=1/10](-5,0) -- (-5,5) -- (5,5) -- (5,0);
    
    \fill (2,3) circle (2pt) node[above]{\footnotesize$z$};
    \draw[](2,3)--(2,1.5) node[midway,right]{\tiny$\dfrac{\Im z}{2}$};
    \filldraw[newblue,fill opacity=1/10,dashed](2,3) circle (1.5);
  \end{tikzpicture}\]
\lecmargin{8}
Let $w \in D_\epsilon(z)$, then \[\abs{w - z} < \epsilon = \frac{\Im z}{2}\]
The end of Discussion \ref{cmplxnorm} tells us
\begin{align*}
\frac{\Im z}{2} > \abs{w - z} &\geq \abs{\Im(w - z)}\\[0.5em]
&= \abs{\Im w - \Im z}
\end{align*}
The later is simply the absolute value of a real number, which gives
\[-\frac{\Im z}{2} < \Im w - \Im z < \frac{\Im z}{2}\]
Adding $\Im z$ throughout the inequality, we get from the inequality on the left hand side
\[\Im w > \frac{\Im z}{2} > 0.\]
Therefore $w \in \mathbf{H}$, and hence $D_\epsilon(z) \subseteq \mathbf{H}$. Thus $\mathbf{H}^\circ = \mathbf{H}$.\\[1em]
The points exterior to $\mathbf{H}$ are points $z$ such that $\Im z < 0$. That is, the exterior of the upper half-plane is the (open) lower half-plane. The boundary of $\mathbf{H}$ consists of precisely points $z$ whose $\Im z = 0$. That is, $\partial \mathbf{H} = \rr$.\\[0.5em]
The closure of $\mathbf{H}$ is $\overline{\mathbf{H}} = \setp{z\in \cc}{\Im z\geq 0}$. While $\mathbf{H} \cup \set{0}$ is neither open nor closed.
\end{itemize}
\end{example}

\medskip

\begin{definition}[Bounded Sets]
A set $S \subseteq \cc$ is \cdef{bounded} if $S \subseteq D_M(0)$ for some $M>0$. That is, there exists an $M>0$ such that $\abs{z} \leq M$ for every $z \in S$.
\end{definition}

\medskip

\begin{definition}[Connected Sets]
A set $S \subseteq \cc$ is said to be \cdef{connected} if each pair of points $z_1$ and $z_2$ in $S$ can be joined by a \emph{polygonal line}, consisting of a finite number of line segments joined end to end, that lies entirely in $S$. Otherwise, we say it is \cdef{disconnected}.
\[\begin{tikzpicture}[scale=0.75]
    \draw[<->,thick] (-2,0)--(5,0);
	\draw[<->,thick] (0,-2)--(0,5);
    \node[] at (4,4) {\color{dirt}$S$};
    \path[draw,use Hobby shortcut,closed=true,fill=dirt,fill opacity=1/10,dashed]
%(-1,0) .. (3,0) .. (3,4) .. (1.5,2) .. (-1,0);
(-2,4) .. (1,2) .. (4,3.5) .. (6,1) .. (5,2) .. (3,0) .. (1.5,-1) .. (1,0) .. (0,1) .. (-2,4);
    \draw[](-1.5,3.5)--(1.5,1);
    \draw[](1.5,1)--(5.5,3.5);
    \draw[](5.5,3.5)--(6.25,2);
    \fill (-1.5,3.5) circle (2pt) node[below left]{$z_1$};
    \fill (6.25,2) circle (2pt) node[below right]{$z_2$};
%    \fill (1.5,1) circle (2pt);
%    \fill (5.5,3.5) circle (2pt);
\end{tikzpicture}\]
\end{definition}

\medskip

\begin{definition}[Domain]
$S \subseteq \cc$ is called a \cdef{domain} if it's a non-empty open and connected set.\\[0.5em]
A \cdef{region} is a domain together with some or all of its boundary points.
\end{definition}

\medskip

\begin{remark}
Domains and regions are sets we will find most suitable for stating elegant results about certain functions in a complex variable.
\end{remark}

\medskip

\begin{example}
$\mathbf{H}$ is a domain since it's non-empty, open and any two points in $\mathbf{H}$ can be connected by a straight line. It's an unbounded set. An example of a region is $\mathbf{H} \cup \set{0}$.
\end{example}

\newpage

\section{Part II. Holomorphic Functions}
\subsection{Complex Functions}

%\begin{mdframed}[backgroundcolor=paleyellow,linewidth=1pt]
%\begin{center}
%{\sc\Large Part II. Holomorphic Functions}
%\end{center}
%\end{mdframed}
%
%\begin{mdframed}
%\begin{center}
%{\Large Complex Functions}
%\end{center}
%\end{mdframed}

\begin{definition}
A \emph{function} $f:G \to \cc$ is a rule that assigns to each $z\in G$ a unique number $f(z) \in \cc$.\\[0.5em]
The set $G$ is called the \emph{domain (of definition)}. If $S \subseteq G$, then
\[f(S) \coloneqq \setp{f(z)}{z\in S}\]
is called the \emph{image of $S$ under $f$}.\\[0.5em]
The set $f(G)$ is called the \emph{image (or range) of $f$}. Points in $f(G)$ are called \emph{values of $f$}.\\[1em]
Given a function $f$, we define its conjugate $\bar{f}$ by the rule $\bar{f}(z) \coloneqq \overline{f(z)}$.
\end{definition}

\medskip

\begin{discussion}
\lecmargin{9}
If $f:G \to \cc$ is a function, then the value $f(x+iy) = u + iv$ depends on a pair $(x,y) \in \rr^2$. Collecting all values, we decompose $f$ into its \cdef{real} and \cdef{imaginary\ parts}
\[f(z) = f(x+iy) = u(x,y) + i\,v(x,y);\quad \Re f = u \ \text{ and } \ \Im f = v,\] where $u,v: \rr^2 \to \rr$ are real-valued functions in two real variables.\\[1em]
In practice, as the examples below tell us, this means replace your $z = x + iy$ and do the required operations to the output $f(x + iy)$. The resulting complex number will be, as a complex number, of the form $u + iv$. The real part is $u$, which you will obtain in terms of $x$ and $y$, and the imaginary part is $v$, which you will also obtain in terms of $x$ and $y$. 
\end{discussion}

\medskip

\begin{example}[Some Complex Functions]\hfill
\begin{itemize}
\item[(1)] $f(z) = z^2 = (x+iy)^2 = (x^2 - y^2) + i(2xy)$. So, \[u(x,y) = x^2 - y^2 \quad \text{and} \quad v(x,y) = 2xy.\]
\item[(2)] $f(z) = \overline{z} = x-iy$. So, \[u(x,y) = x \quad \text{and} \quad v(x,y) = -y.\]
\item[(3)] (in-class) $f(z) = z\overline{z} = \abs{z}^2 = x^2+y^2$. So, \[u(x,y) = x^2 + y^2 \quad \text{and} \quad v(x,y) = 0.\]
Such a function is \emph{real-valued}.
\item[(4)] \emph{Polynomials of degree $n$} are functions of the form \[p(z) = a_0 + a_1z + \cdots a_nz^n,\] where $a_i \in \cc$ and $a_n \neq 0$.\\[1em]
A polynomial of degree $0$ is simply a non-zero complex number, sometimes also referred to as a \emph{constant polynomial}.
\item[(5)] \emph{Rational functions (or polynomials)} are functions of the form
\[\dfrac{p(z)}{q(z)}\]
where $p(z)$ and $q(z)$ are polynomials. The domain of definition is wherever $q(z) \neq 0$. For example,
\[f:\cc^* \to \cc,\ z \mapsto \frac{1}{z}\]
\item[(6)] If we express $z$ in its polar form, then a function $f$, when we restrict its domain of definition within $\cc^*$, can be written as
\[f(z) = f(re^{i\theta}) = u(r,\theta) + i\,v(r,\theta)\]
For example,
\[f:\cc^* \to \cc,\ z = re^{i\theta} \mapsto \frac{1}{z} = \frac{1}{r}e^{-i\theta} = \frac{\cos\theta}{r} - i\,\frac{\sin\theta}{r}.\]
Here $u(r,\theta) = \dfrac{\cos\theta}{r}$ and $v(r,\theta) = -\dfrac{\sin\theta}{r}$.
\item[(7)] (in-class) Let's consider the function $f(z) = \overline{z}^2$, in polar form we have
\begin{align*}
f(re^{i\theta}) &= (\overline{re^{i\theta}})^2\\[0.5em]
&= (re^{-i\theta})^2,\quad \text{by Proposition \ref{propeuler} (4)}\\[0.5em]
&= r^2e^{-i2\theta},\quad \text{by Proposition \ref{propeuler} (3)}\\[0.5em]
&= r^2(\cos(-2\theta) + i\sin(-2\theta))\\[0.5em]
&= r^2\cos(2\theta) - i\sin(2\theta))
\end{align*}
Therefore, here $u(r,\theta) = r^2\cos(2\theta)$ and $v(r,\theta) = -r^2\sin(2\theta)$.
\item[(8)] Consider $f(z) = z^{1/n}$, where $n$ is a non-zero integer. For no $n \neq 1$ is this a function! We have seen previously that $z^{1/n}$ has $n$-distinct values. Such a "function" is called multi-valued.\\[1em]
We can make this into a (single-valued) function by assigning a single value of $z^{1/n}$ to each $z$; taking the \emph{principal $n^{\text{th}}$ root of $z$}, for instance. More on such functions soon. 
\end{itemize}
\end{example}

\bigskip

\subsection{Limits of Functions}
%\begin{mdframed}
%\begin{center}
%{\Large Limits of Functions}
%\end{center}
%\end{mdframed}

\begin{definition}[Limit of a Function]\label{limdef}
Consider a function $f:G \to \cc$, and an accumulation point $z_0$ of $G$.\\[0.5em]
We say that \cdef{limit} of $f$, as $z$ approaches $z_0$, is $w_0 \in \cc$ if \emph{for all $\epsilon > 0$ there exists a $\delta > 0$ such that
\[\text{if }\ 0 < \abs{z - z_0} < \delta,\quad \text{then }\ \abs{f(z) - w_0}<\epsilon\]}\\
Equivalently, if $z \in D_{\delta}(z_0)\setminus \set{z_0}$, then $f(z) \in D_\epsilon(w_0)$.
\[\begin{tikzpicture}[scale=0.85]
    \draw[<->,thick] (-1,0)--(5,0);
	\draw[<->,thick] (0,-1)--(0,5);
	\filldraw[firebrick,fill opacity=1/7,dashed](3,3) circle (2);
    \draw[firebrick,dotted](3,3) circle (3pt);    
    \draw[](3,3)--(4.86,3.735) node[sloped,midway,above]{$\delta$};
    \fill[white] (3,3) circle (3pt) node[below,yshift=-1pt]{\color{black}$z_0$};
    \fill (2,3.735) circle (2pt) node[left]{$z$};
  \end{tikzpicture}
  \qquad\qquad\qquad
  \begin{tikzpicture}[scale=0.85]
    \draw[<->,thick] (-1,0)--(5,0);
	\draw[<->,thick] (0,-1)--(0,5);
	\filldraw[teal,fill opacity=1/7,dashed](2,2) circle (1.5);
    \draw[](2,2)--(3.5,2) node[sloped,midway,above]{$\epsilon$};
    \fill (2,2) circle (2pt) node[above left]{$w_0$};
    \fill (1.2,1.2) circle (2pt) node[right]{$f(z)$};
  \end{tikzpicture}\]
In this case we write $\lim_{z \to z_0}f(z) = w_0$ or $f(z) \to w_0,\ \text{as } z \to z_0$.\\
\\
Intuitively, the limit of $f$ at $z_0$ is $w_0$ if \[\text{"$f$ is arbitrarily close to $w_0$ eventually, that is sufficiently, near $z_0$".}\] How close? Within an error of $\epsilon$. How near, eventually? Within a distance of $\delta$.
\end{definition}

\bigskip

\medskip

\begin{example}
\lecmargin{10}
Let's show that $\lim_{z \to i} z^2 = -1$ using the definition.
\end{example}
\begin{proof}
Let $\epsilon > 0$ be arbitrary. Note that $\abs{z^2 - (-1)} = \abs{z - i}\abs{z + i}$. We make an initial estimate, suppose $0 < \abs{z - i} < 1$, then
\begin{align*}
\abs{z + i} &= \abs{z - i + 2i}\\[0.5em]
&\leq \abs{z - i} + \abs{2i}\\[0.5em]
&< 1 + 2\\[0.5em]
&= 3
\end{align*}
Now, if we choose $\delta = \min\set{\dfrac{\epsilon}{3},1}$, then if $0 < \abs{z - i} < \delta$ we get
\[0 < \abs{z - i} < 1\ \text{and}\ \frac{\epsilon}{3}\]
So,
\begin{align*}
\abs{z^2 - (-1)} &= \abs{z - i}\abs{z + i}\\[0.5em]
&< 3\abs{z - i},\quad \text{since $\abs{z - i} < 1$}\\[0.5em]
&< 3\cdot\frac{\epsilon}{3},\quad \text{since $\abs{z - i} < \frac{\epsilon}{3}$}\\[0.5em]
&= \epsilon
\end{align*}
Therefore $\lim_{z \to i} z^2 = -1$.
\end{proof}

\medskip

\begin{theorem}\label{limunique}
If $f$ has a limit at $z_0$, then it is unique.
\end{theorem}
\begin{proof}
Assume
\[\lim_{z \to z_0}f(z) = \alpha \quad \text{and} \quad \lim_{z \to z_0}f(z) = \beta\]
Consider an arbitrary $\epsilon > 0$, then we can find a $\delta_1 > 0$ such that
\[\text{if }\ 0 < \abs{z - z_0} < \delta,\quad \text{then }\ \abs{f(z) - \alpha}<\frac{\epsilon}{2}\]
and $\delta_2 > 0$ such that
\[\text{if }\ 0 < \abs{z - z_0} < \delta,\quad \text{then }\ \abs{f(z) - \beta}<\frac{\epsilon}{2}\]
Define $\delta \coloneqq \min\set{\delta_1,\delta_2} \leq \delta_1,\,\delta_2$, then if $0< \abs{z - z_0} < \delta$ we have
\begin{align*}
\abs{\alpha - \beta} &= \abs{f(z) - f(z) + \alpha - \beta}\\[0.5em]
&\leq \abs{\alpha - f(z)} + \abs{f(z) - \beta}\\[0.5em]
&= \abs{f(z) - \alpha} + \abs{f(z) - \beta}\\[0.5em]
&< \frac{\epsilon}{2} + \frac{\epsilon}{2}\\[0.5em]
&= \epsilon
\end{align*}
We have proven that $\abs{\alpha - \beta} < \epsilon$ for any $\epsilon > 0$. Now, suppose $\alpha \neq \beta$, then for $\epsilon = \abs{\alpha - \beta} > 0$ we get $\abs{\alpha - \beta} < \abs{\alpha - \beta}$, which is preposterous. Hence $\alpha = \beta$, and thus the limit is unique.
\end{proof}

\medskip

\begin{remark}
The reason we require that $z_0$ be an accumulation point of the domain of $f$ is just that we need to be sure that there are points $z$ of the domain that are arbitrarily close to $z_0$. That is, there are indeed points satisfying $0< \abs{z-z_0} < \delta$.\\[0.5em]
Our definition (i.e., the part that says $0 < \abs{z - z_0}$) does not require $z_0$ to be in the domain of $f$, and if $z_0$ is in the domain of $f$, the definition explicitly ignores the value of $f(z_0)$.
\end{remark}

\medskip

Uniqueness of limits can be used to show that a limit does not exist.
\begin{example}\label{limnotex}
The function $f(z) = \dfrac{\overline{z}}{z}$ has no limit at $0$.
\end{example}
\begin{proof}[Discussion of Example \ref{limnotex}]
Let $z = x + iy$, then
\[f(z) = \frac{x - iy}{x + iy}\]
Along the real axis, $\Im z = 0$, and so $z = x$, giving us $f(z) = \dfrac{x}{x} = 1$.\\[0.5em]
Along the imaginary axis, $\Re z = 0$, and so $z = y$, giving us $f(z) = \dfrac{-y}{y} = -1$.\\[0.5em]
Taking the limit along these axes gives us different values of the limit, $1$ and $-1$. Hence, by the uniqueness of limits, the limit doesn't exist.
\end{proof}

\bigskip

\subsection{Theorems on Limits}
%\begin{mdframed}
%\begin{center}
%{\Large Theorems on Limits}
%\end{center}
%\end{mdframed}

\begin{theorem}[Limit in terms of Real and Imaginary parts of a Function]\label{cmplxlimripart}
Suppose that
\[f(z) = f(x + iy) = u(x,y) + i\,v(x,y)\]
Then 
\[\lim_{x + iy \to x_0 + iy_0}f(x + iy) = u_0 + iv_0\]
if and only if
\[\lim_{(x,y) \to (x_0,y_0)}u(x,y) = u_0 \quad \text{and} \quad \lim_{(x,y) \to (x_0,y_0)}v(x,y) = v_0\]
\end{theorem}
\begin{proof}
$(\Rightarrow)$ Consider an arbitrary $\epsilon > 0$, then there exists a $\delta > 0$ such that if
\[0 < \abs{(x+iy) - (x_0 + iy_0)} < \delta\]
\[\text{then }\ \abs{f(x+iy) - (u_0 + iv_0)} = \abs{(u(x,y) + i\,v(x,y)) - (u_0 + iv_0)} < \epsilon\]
We first note that, by definition
\begin{align*}
\norm{(x,y) - (x_0,y_0)} &= \abs{(x+iy) - (x_0 + iy_0)}
\end{align*}
and the end of Discussion \ref{cmplxnorm} tells us that
\begin{align*}
\abs{u(x,y) - u_0} &\leq \abs{(u(x,y) + i\,v(x,y)) - (u_0 + iv_0)} < \epsilon\\[0.5em]
\abs{v(x,y) - v_0} &\leq \abs{(u(x,y) + i\,v(x,y)) - (u_0 + iv_0)} < \epsilon
\end{align*}
That is, we have that
\[\text{if }\ 0 < \norm{(x,y) - (x_0,y_0)} < \delta,\quad \text{then }\ \abs{u(x,y) - u_0} < \epsilon\ \ \text{and}\ \ \abs{v(x,y) - v_0} < \epsilon\]
Therefore,
\[\lim_{(x,y) \to (x_0,y_0)}u(x,y) = u_0 \quad \text{and} \quad \lim_{(x,y) \to (x_0,y_0)}v(x,y) = v_0\]\\[1em]
$(\Leftarrow)$ Consider an arbitrary $\epsilon > 0$, then there exists a $\delta_1 > 0$ such that
\[\text{if }\ 0 < \norm{(x,y) - (x_0,y_0)} < \delta_1,\quad \text{then }\ \abs{u(x,y) - u_0} < \frac{\epsilon}{2}\]
and there exists a $\delta_2 > 0$ such that
\[\text{if }\ 0 < \norm{(x,y) - (x_0,y_0)} < \delta_2,\quad \text{then }\ \abs{v(x,y) - v_0} < \frac{\epsilon}{2}\]
Define $\delta \coloneqq \min\set{\delta_1,\delta_2} \leq \delta_1,\,\delta_2$. Now, if
\[0 < \abs{(x+iy) - (x_0 + iy_0)} = \norm{(x,y) - (x_0,y_0)} < \delta\]
then
\begin{align*}
\abs{f(x+iy) - (u_0 + iv_0)} &= \abs{(u(x,y) + i\,v(x,y)) - (u_0 + iv_0)}\\[0.5em]
&= \abs{(u(x,y) - u_0) + i(v(x,y)- v_0)}\\[0.5em]
&\leq \abs{(u(x,y) - u_0)} + \abs{i(v(x,y)- v_0)},\ \text{by triangle identity}\\[0.5em]
&= \abs{(u(x,y) - u_0)} + \abs{i}\abs{(v(x,y)- v_0)}\\[0.5em]
&= \abs{(u(x,y) - u_0)} + \abs{(v(x,y)- v_0)}\\[0.5em]
&< \frac{\epsilon}{2} + \frac{\epsilon}{2}\\[0.5em]
&= \epsilon
\end{align*}
Therefore,
\[\lim_{x + iy \to x_0 + iy_0}f(x + iy) = u_0 + iv_0\]
\end{proof}

\medskip

\begin{theorem}[Limit Laws]\label{limlaw}
\lecmargin{11}
Suppose
\[\lim_{z \to z_0} f(z) = \alpha \quad \text{and} \quad \lim_{z \to z_0} g(z) = \beta\]
Then
\begin{itemize}
\item[(1)] $\lim_{z \to z_0} (f(z) + g(z)) = \alpha + \beta$
\item[(2)] $\lim_{z \to z_0} (f(z)\,g(z)) = \alpha\beta$
\item[(3)] $\lim_{z \to z_0} \dfrac{f(z)}{g(z)} = \dfrac{\alpha}{\beta}$, provided $\beta \neq 0$.
\end{itemize}
\end{theorem}
\begin{proof}
The proof follows from Theorem \ref{cmplxlimripart} and limit laws from Calculus.
\end{proof}

\medskip

\begin{example}\label{polycts}
Let $p(z)$ be a polynomial, then
\[\lim_{z \to z_0}p(z) = p(z_0)\]
Write $p(z) = a_0 + a_1z + \cdots + a_nz^n$, then by Theorem \ref{limlaw} we have
\begin{align*}
\lim_{z \to z_0}p(z) &= \lim_{z \to z_0}(a_0 + a_1z + \cdots + a_nz^n)\\[0.5em]
&= \lim_{z \to z_0} a_0 + \lim_{z \to z_0} a_1z + \cdots + \lim_{z \to z_0} a_nz^n,\ \text{by Theorem \ref{limlaw} (1)}\\[0.5em]
&= \lim_{z \to z_0} a_0 + \lim_{z \to z_0} a_1 \cdot \lim_{z \to z_0} z + \cdots + \lim_{z \to z_0} a_n \cdot \lim_{z \to z_0} z^n,\ \text{by Theorem \ref{limlaw} (2)}\\[0.5em]
&= a_0 + a_1z_0 + \cdots + a_nz_0^n,\ \text{by Theorem \ref{limlaw} (2) and }\lim_{z \to z_0} z = z_0\\[0.5em]
&= p(z_0)
\end{align*}\\[-2em]
\qed
\end{example}

\bigskip

\begin{definition}[Extended Complex Plane or the Riemann Sphere]
The \cdef{Extended\ Complex\ Plane} is the set $\cc$ together with a symbol $\infty$ called the \emph{point at infinity}, denoted $\widehat{\cc}$ or $\cc_\infty$.\\[0.5em]
There is a bijection between the extended complex plane and the unit sphere given by the \emph{stereographic projection}, and therefore the extended complex plane is also called the \cdef{Riemann\ Sphere}.\\
\[\begin{tikzpicture}
  \draw[dashed] (-6,-1.5) -- (4.5, -1.5) -- (6, 1.5) -- (-3.5,1.5) -- (-6,-1.5);
  \draw (0,0) circle (3);
  \draw[dashed] (0,0) ellipse (3 and 1);
  \draw[] (0,3) -- (4.5, -1);
  \fill[forest] (0,3) circle (2pt) node[above,yshift=2pt]{{\color{forest}$N$}};
  \fill (0, 0) circle (2pt);
  \fill (4.5, -1) circle (2pt) node[below left]{$z$};
  \fill (0.56, 2.502) circle (2pt) node[right,yshift=3pt,xshift=2pt]{\small$p$};
  \shade[fill=forest,fill opacity=1/8] (5,1.5) arc (0:-60:3.45) -- (4.5,-1.5) -- (6,1.5) -- (5,1.5);
  \draw[dashed] (5,1.5) arc (0:-60:3.45);
  \shade[fill=forest,fill opacity=1/8] (-4.75,-1.5) arc (0:-75:-3.1) -- (-3.5,1.5) -- (-6,-1.5) -- (-4.75,-1.5);
  \draw[dashed] (-4.75,-1.5) arc (0:-75:-3.1);
  \shade[fill=forest,fill opacity=1/8] (2,2.236) arc (113.5:66.5:-5) arc (-48.35:-131.8:-3);
  \draw[dashed] (2,2.236) arc (113.5:66.5:-5);
  \node[] at (4,2) {\Large$\cc$};
\end{tikzpicture}\]\\
The point $N$ (the north pole) corresponds to $\infty$, and any point $p$ on the sphere corresponds uniquely to a point $z \in \cc$ which is the unique point of intersection of the complex plane with the line passing through $N$ and $p$.
\end{definition}

\medskip

\begin{definition}[Neighbourhood of Infinity]
Let $\epsilon > 0$, the set
\[\setp{z\in \cc}{\abs{z} > \frac{1}{\epsilon}}\]
is called a \emph{neighbourhood of $\infty$}. Geometrically, a neighbourhood at infinity is the exterior of a circle centered at the origin, which corresponds to a neighbourhood of $N$ on the unit sphere.
\end{definition}

\medskip

\begin{discussion}
We can now easily give meaning to limits
\[\lim_{z \to z_0}f(z) = w_0\]
where $z_0$ and $w_0$ are allowed to be $\infty$. We replace the appropriate neighbourhood in  Definition \ref{limdef} with neighbourhoods of $\infty$.
\end{discussion}

\medskip

\begin{theorem}[Limits involving Infinity]\hfill
\begin{itemize}
\item[(1)] $\displaystyle\lim_{z \to z_0} \dfrac{1}{f(z)} = 0$ if and only if $\displaystyle\lim_{z \to z_0}f(z) = \infty$.
\item[(2)] $\displaystyle\lim_{z \to \infty}f(z) = \lim_{z \to 0} f\left(\dfrac{1}{z}\right)$, provided the limit exist.
\end{itemize}
\emph{Combining (1) and (2), we get \[\displaystyle\lim_{z \to 0} \dfrac{1}{f\left(\frac{1}{z}\right)} = 0\quad \text{if and only if} \quad \displaystyle\lim_{z \to \infty}f(z) = \infty.\]}
Bottom line, we can simplify limits involving $\infty$ to limits involving $0$.
\end{theorem}
\begin{proof}
The proofs are based on the simple observation that
\[\frac{1}{a} < b \quad \text{if and only if} \quad \frac{1}{b} < a\]
for non-zero real numbers $a$ and $b$.
\begin{itemize}
\item[(1)] Now $\displaystyle\lim_{z \to z_0} \dfrac{1}{f(z)} = 0$ if and only if for every $\epsilon > 0$ there exists $\delta > 0$ such that 
\[\text{if }\ 0 < \abs{z - z_0} < \delta,\quad \text{then }\ \frac{1}{\abs{f(z)}} = \abs{\frac{1}{f(z)} - 0}< \epsilon\]
if and only if for every $\epsilon > 0$ there exists $\delta > 0$ such that 
\[\text{if }\ 0 < \abs{z - z_0} < \delta,\quad \text{then }\ \abs{f(z)}>\frac{1}{\epsilon}\]
if and only if
$\displaystyle\lim_{z \to z_0}f(z) = \infty$.
\item[(2)] $\displaystyle\lim_{z \to \infty}f(z) = \alpha$ if and only if for every $\epsilon > 0$ there exists $\delta > 0$ such that 
\[\text{if }\ \abs{z} > \frac{1}{\delta},\quad \text{then }\ \abs{f(z) - \alpha}< \epsilon\]
if and only if for every $\epsilon > 0$ there exists $\delta > 0$ such that 
\[\text{if }\ 0 < \abs{\frac{1}{z}} < \delta,\quad \text{then }\ \abs{f(z) - \alpha} < \epsilon\]
if and only if, by replacing $z$ with $1/z$, $\displaystyle\lim_{z \to 0}\,f\left(\dfrac{1}{z}\right) = \alpha$.
\end{itemize}
\end{proof}

\medskip

\begin{example}
We want to show $\displaystyle \lim_{z \to \infty} \frac{2z^4 + 1}{z^3 + 1} = \infty$. This is equivalent to showing
\[\lim_{z \to 0} \frac{1}{f(1/z)} = \lim_{z \to 0} \frac{(1/z)^3 + 1}{2(1/z)^4 + 1} = 0,\quad \text{for }f(z) = \frac{2z^4 + 1}{z^3 + 1}\]
Note that,
\begin{align*}
\lim_{z \to 0} \frac{(1/z)^3 + 1}{2(1/z)^4 + 1}&= \lim_{z \to 0} \frac{\frac{1 + z^3}{z^3}}{\frac{2 + z^4}{z^4}}\\[0.5em]
&= \lim_{z \to 0}\ z\cdot\frac{1 + z^3}{2 + z^4}\\[0.5em]
&= 0\cdot\frac{1}{2}\\[0.5em]
&= 0
\end{align*}
Therefore $\displaystyle \lim_{z \to \infty} \frac{2z^4 + 1}{z^3 + 1} = \infty$.
\end{example}

\medskip

\begin{example}[in-class]
Show $\displaystyle \lim_{z \to \infty} \frac{2 + z^5}{z^2 + 3} = \infty$.
\end{example}
\begin{proof}[Answer]
This is equivalent to showing
\[\lim_{z \to 0} \frac{1}{f(1/z)} = \lim_{z \to 0} \frac{(1/z)^2 + 3}{2 + (1/z)^5} = 0,\quad \text{for }f(z) = \frac{2 + z^5}{z^2 + 3}\]
Note that,
\begin{align*}
\lim_{z \to 0} \frac{(1/z)^2 + 3}{2 + (1/z)^5} &= \lim_{z \to 0} \frac{\frac{1 + 3z^2}{z^2}}{\frac{2z^5 + 1}{z^5}}\\[0.5em]
&= \lim_{z \to 0}\ z^3\cdot\frac{1 + 3z^2}{2z^5 + 1}\\[0.5em]
&= 0^3\cdot\frac{1}{1}\\[0.5em]
&= 0
\end{align*}
Therefore $\displaystyle \lim_{z \to \infty} \frac{2 + z^5}{z^2 + 3} = \infty$.
\end{proof}

\bigskip

\subsection{Continuous Functions}
%\begin{mdframed}
%\begin{center}
%{\Large Continuous Functions}
%\end{center}
%\end{mdframed}

\begin{definition}[Continuous Functions]
A function $f: G \to \cc$ is \emph{continuous at $z_0 \in G$} if either $z_0$ is an isolated point or 
\[\lim_{z \to z_0}\,f(z) = f(z_0) = f\left(\lim_{z\to z_0}\,z\right)\]
That is, for all $\epsilon > 0$, there exists a $\delta > 0$ such that
\[\text{if }\ 0 < \abs{z - z_0} < \delta,\quad \text{then }\ \abs{f(z) - f(z_0)}<\epsilon.\]
A function is \cdef{continuous} if it is continuous at every point in its domain.\\
\\
By the limit laws (Theorem \ref{limlaw}), sum, product and quotient of continuous functions are continuous (whenever and wherever defined).
\end{definition}

\vspace*{1em}

\begin{theorem}[Composition of Continuous Functions]\label{composcont}
Suppose we have two functions $f:G_1 \to \cc$ and $g: G_2 \to \cc$ such that $f(G_1) \subseteq G_2$. If $f$ is continuous at $z_0$ and $g$ is continuous at $f(z_0)$, then $g\circ f$ is continuous at $z_0$. That is,
\[\lim_{z \to z_0}g(f(z)) = g(f(z_0)) = g\left(\lim_{z\to z_0}f(z)\right) = g\left(f\left(\lim_{z\to z_0}\,z\right)\right)\]
Therefore, if $f$ and $g$ are continuous, so is $g\circ f$.
\end{theorem}
\begin{proof}
By continuity of $g$ at $f(z_0)$, for an arbitrary $\epsilon > 0$, there exists a $\delta_1 > 0$ such that
\[\text{if }\ 0 < \abs{w - f(z_0)} < \delta_1,\quad \text{then }\ \abs{g(w) - g(f(z_0))}<\epsilon.\]
Now, by continuity of $f$ at $z_0$, for $\delta_1 > 0$, there exists a $\delta > 0$ such that
\[\text{if }\ 0 < \abs{z - z_0} < \delta,\quad \text{then }\ \abs{f(z) - f(z_0)}<\delta_1.\]
With these two statements, we have that 
\[\text{if }\ 0 < \abs{z - z_0} < \delta,\quad \text{then }\ \abs{g(f(z)) - g(f(z_0))}<\epsilon.\]
Therefore $g\circ f$ is continuous at $z_0$.
\end{proof}

\vspace*{1em}

\begin{theorem}
Suppose $f: G \to \cc$ is continuous at $z_0$ and $f(z_0) \neq 0$, then there exists a $\delta > 0$ such that $f(z) \neq 0$ for all $z \in D_\delta(z_0)$. That is, $\abs{f(z)} > 0$ for all $z \in D_\delta(z_0)$.
\end{theorem}
\begin{proof}
Since $f$ is continuous and non-zero at $z_0$, for $\epsilon = \dfrac{\abs{f(z_0)}}{2} > 0$ there exists a $\delta > 0$ such that
\[\text{if }\ z \in D_\delta(z_0),\quad \text{then }\ \abs{f(z) - f(z_0)}< \frac{\abs{f(z_0)}}{2}.\]
For such a $z$, the reverse triangle inequality gives us
\[\abs{\abs{f(z)} - \abs{f(z_0)}} \leq \abs{f(z) - f(z_0)}< \frac{\abs{f(z_0)}}{2};\quad \text{so, }\ -\frac{\abs{f(z_0)}}{2} < \abs{f(z)} - \abs{f(z_0)} < \frac{\abs{f(z_0)}}{2}\]
since the former is the absolute value of real numbers. Therefore, adding $\abs{f(z_0)}$ to this inequality gives us
\[\abs{f(z)} > \frac{\abs{f(z_0)}}{2} > 0\]
as needed.
\end{proof}

\vspace*{1em}

\begin{theorem}[Continuity in terms of Real and Imaginary parts of a Function]\label{contpart}
Suppose that 
\[f(z) = f(x + iy) = u(x,y) + i\,v(x,y).\]
Then $f$ is continuous at $z_0 = x_0 + iy_0$ if and only if $u$ and $v$ are continuous at $(x_0,y_0)$.
\end{theorem}
\begin{proof}
This is directly follows from Theorem \ref{cmplxlimripart}.
\end{proof}

\vspace*{1em}

\begin{definition}[Compact Sets]
A subset of $\cc$ is said to be \cdef{compact} if it is closed and bounded.
\end{definition}

\vspace*{1em}

\begin{definition}[Bounded Functions]
A function $f: G \to \cc$ is said to be a \cdef{bounded\ function} if the image $f(G)$ is bounded. Equivalently, if there exists $M > 0$ such that $\abs{f(z)} \leq M$ for every $z \in G$.
\end{definition}

\vspace*{1em}

\begin{theorem}[Extreme Value Theorem]\label{evt}
Suppose $K \subseteq \cc$ is compact, and $f: K \to \cc$ is continuous. Then $f$ is bounded, that is there exists an $M > 0$ such that $\abs{f(z)} \leq M$ for all $z \in K$, and there exists a $z_0 \in K$ such that $\abs{f(z_0)} = M$.
\end{theorem}
\begin{proof}
Since $f = u + iv$ is continuous, so are $u,v:\rr^2 \to \rr$ by Theorem \ref{contpart}.  Hence, so is
\[\abs{f(z)} = \abs{f(x + iy)} = \sqrt{u(x,y)^2 + v(x,y)^2}\]
as it's obtained as a sum, product and composition of continuous functions. This result then follows from standard Calculus, since $\abs{f}$ is a real-valued function.
\end{proof}

\bigskip

\subsection{Complex-Differentiable Functions}
%\begin{mdframed}
%\begin{center}
%{\Large Complex-Differentiable Functions}
%\end{center}
%\end{mdframed}

\begin{definition}[Derivative]\label{cmplxder}
Consider a function $f:G \to \cc$, the \cdef{derivative} \emph{of $f$ at $z_0 \in G$} is the limit
\[\frac{d}{dz}(f(z_0)) = f'(z_0) \coloneqq \lim_{z \to z_0}\frac{f(z) - f(z_0)}{z - z_0}\]
If the limit exists, we say $f$ is \emph{differentiable at $z_0$}.\\[0.5em]
A function is \cdef{differentiable} if it is differentiable at every point in its domain.
\end{definition}
Letting $h = \Delta_{z_0}z = z - z_0$, the limit can also be written as
\[f'(z_0) = \lim_{h \to 0}\frac{f(z_0 + h) - f(z_0)}{h}\]

\vspace*{1em}

\begin{example}
Consider $f(z) = z^2$, then
\begin{align*}
f'(z) = \lim_{h \to 0}\frac{f(z + h) - f(z)}{h} &= \lim_{h \to 0}\frac{(z + h)^2 - z^2}{h}\\[0.5em]
&= \lim_{h \to 0}\frac{2zh + h^2}{h}\\[0.5em]
&= \lim_{h \to 0}\,2z + h\\[0.5em]
&= 2z
\end{align*}
\end{example}

\vspace*{1em}

\begin{example}\label{normdiffexistence}
Where is $f(z) = \abs{z}^2$ differentiable?\\[1em]
Consider $z \in \cc$ and an arbitrary $h \in \cc$, then we compute
\begin{align*}
f(z + h) - f(z) &= \abs{z + h}^2 - \abs{z}^2\\[0.5em]
&= (z + h)\overline{(z + h)} - z\overline{z}\\[0.5em]
&= z\overline{z} + z\overline{h} + \overline{z}h + h\overline{h} - z\overline{z}\\[0.5em]
&= z\overline{h} + \overline{z}h + h\overline{h}
\end{align*}
Then 
\[\frac{f(z + h) - f(z)}{h} = \frac{z\overline{h} + \overline{z}h + h\overline{h}}{h} = z\,\frac{\overline{h}}{h} + \overline{z} + \overline{h}\]
Along the real axis, $h = \overline{h}$, we have
\[\frac{f(z + h) - f(z)}{h} = z + \overline{z} + h;\]
therefore, as $h \to 0$, the limit is $z + \overline{z}$. Along the imaginary axis, $h = -\overline{h}$, we have
\[\frac{f(z + h) - f(z)}{h} = -z + \overline{z} - h;\]
therefore, as $h \to 0$, the limit is $-z + \overline{z}$.\\
\\
Since limits are unique, if $f'(z)$ exists, then $z + \overline{z} = -z + \overline{z}$, which gives us $z = 0$. That is, if $f'(z)$ exists, it only exists for $z = 0$. So, does $f'(0)$ exist?
\[f'(0) = \lim_{h \to 0}\frac{f(h) - f(0)}{h} = \lim_{h \to 0}\frac{h\overline{h}}{h} = \lim_{h \to 0}\overline{h} = 0\]
\end{example}

\vspace*{1em}

\begin{proposition}[Differentiable Functions are Continuous]
If $f$ is differentiable at $z_0$, then $f$ is continuous at $z_0$.
\end{proposition}
\begin{proof}
Suppose $f$ is differentiable at $z_0$, then
\[\lim_{z \to z_0}f(z) - f(z_0) = \left(\lim_{z \to z_0}\frac{f(z) - f(z_0)}{z - z_0}\right)\left(\lim_{z \to z_0}\,z - z_0\right) = f'(z_0)\cdot 0 = 0\]
Therefore $\lim_{z \to z_0}f(z) = f(z_0)$, and hence $f$ is continuous at $z_0$.
\end{proof}

\vspace*{1em}

\begin{theorem}[Differentiation Laws]
Suppose $f$ and $g$ are differentiable at $z$. Then,
\begin{itemize}[itemsep=1em]
\item[(1)] $(c)' = 0$, for every $c \in \cc$.
\item[(2)] $(c\cdot f)'(z) = c\cdot f'(z)$, for every $c \in \cc$.\hfill \emph{(Constant Rule)}
\item[(3)] $(z^n)' = nz^{n-1}$, for every $n \in \zz$ (assume $z \neq 0$ for $n<0$).\hfill \emph{(Power Rule)}
\item[(4)] $(f + g)'(z) = f'(z) + g'(z)$.\hfill \emph{(Sum Rule)}
\item[(5)] $(fg)'(z) = f'(z)g(z) + f(z)g'(z)$.\hfill \emph{(Product Rule)}
\item[(6)] $\left(\dfrac{f}{g}\right)'(z) = \dfrac{f'(z)g(z) - f(z)g'(z)}{g(z)^2}$, provided $g(z) \neq 0$ \hfill \emph{(Quotient Rule)}
\end{itemize}
\end{theorem}
\begin{proof}
(1) and (4) are proved directly using the limit definition, (2) can be proved directly or using (1) and (5), while (3) can be proven inductively using (5) for positive $n$ and (6) for negative $n$. 
\begin{itemize}
\item[(5)] We first compute
\begin{align*}
f(z + h)g(z + h) - f(z)g(z) &= f(z + h)g(z + h) - f(z)g(z) + f(z + h)g(z) - f(z + h)g(z)\\[0.5em]
&= f(z + h)(g(z + h) - g(z)) + g(z)(f(z + h) - f(z))
\end{align*}
So,
\begin{align*}
(fg)'(z) &= \lim_{h \to 0}\frac{f(z + h)g(z + h) - f(z)g(z)}{h}\\[0.5em]
&= \lim_{h \to 0}\frac{f(z + h)(g(z + h) - g(z))}{h} + \lim_{h \to 0}\frac{g(z)(f(z + h) - f(z))}{h}\\[0.5em]
&= \lim_{h \to 0}f(z + h)\cdot\lim_{h \to 0}\frac{g(z + h) - g(z)}{h} + g(z)\lim_{h \to 0}\frac{f(z + h) - f(z)}{h}\\[0.5em]
&= f(z)g'(z) + g(z)f'(z)
\end{align*}
\item[(6)] We first compute
\begin{align*}
\frac{1}{g(z + h)} - \frac{1}{g(z)} &= \frac{g(z) - g(z + h)}{g(z)g(z + h)}\\[0.5em]
&= -\frac{g(z+h) - g(z)}{g(z)g(z + h)}
\end{align*}
So,
\begin{align*}
\left(\dfrac{1}{g}\right)'(z) &= \lim_{h \to 0}\frac{\dfrac{1}{g(z + h)} - \dfrac{1}{g(z)}}{h}\\[0.5em]
&= \lim_{h \to 0}-\frac{g(z+h) - g(z)}{g(z)g(z + h)}\cdot\frac{1}{h}\\[0.5em]
&= -\lim_{h \to 0}\frac{g(z+h) - g(z)}{h}\cdot\lim_{h \to 0}\frac{1}{g(z)g(z + h)}\\[0.5em]
&= -\frac{g'(z)}{g(z)^2}
\end{align*}
(6) then follows from the computation above and using (5) on $\dfrac{f(z)}{g(z)} = f(z)\cdot\dfrac{1}{g(z)}$.
\end{itemize}
\vspace*{-\baselineskip}
\end{proof}

\vspace*{1em}

\begin{proposition}[Chain Rule]
Suppose we have two functions $f:G_1 \to \cc$ and $g: G_2 \to \cc$ such that $f(G_1) \subseteq G_2$. If $f$ is differentiable at $z_0$ and $g$ is differentiable at $f(z_0)$, then $g\circ f$ is differentiable at $z_0$ and
\[(g\circ f)'(z_0) = g'(f(z_0))\cdot f'(z_0)\]
\end{proposition}
\begin{proof}
Let's start by defining an auxiliary function on $G_2$
\[\phi(w) = \begin{cases} \dfrac{g(w) - g(f(z_0))}{w - f(z_0)} - g'(f(z_0)) & w \neq f(z_0)\\[0.5em] 0 & w = f(z_0) \end{cases}\]
Since $g$ is differentiable at $f(z_0)$, then $\lim_{w \to f(z_0)}\phi(w) = 0 = \phi(f(z_0))$ and therefore $\phi$ is continuous at $f(z_0)$. Furthermore, since $f$ is differentiable at $z_0$, it is continuous at $z_0$. So $\lim_{z \to z_0}\phi(f(z)) = \phi(f(z_0)) = 0$ by Theorem \ref{composcont}.\\[1em]
Rewriting the above expression, we get the following expression which is valid on all of $G_2$.
\[g(w) - g(f(z_0)) = (w - f(z_0))(\phi(w) + g'(f(z_0)))\]
Now, for $w = f(z) \in f(G_1)$, we have
\begin{align*}
\frac{g(f(z)) - g(f(z_0))}{z - z_0} &= \frac{(f(z) - f(z_0))(\phi(f(z)) + g'(f(z_0)))}{z - z_0}\\[0.5em]
&= (\phi(f(z)) + g'(f(z_0)))\cdot\frac{f(z) - f(z_0)}{z - z_0}
\end{align*}
Therefore, 
\begin{align*}
(g\circ f)'(z_0) &= \lim_{z \to z_0}\frac{g(f(z)) - g(f(z_0))}{z - z_0}\\[0.5em]
&= \lim_{z \to z_0}(\phi(f(z)) + g'(f(z_0)))\cdot\lim_{z \to z_0}\frac{f(z) - f(z_0)}{z - z_0}\\[1em]
&= g'(f(z_0))\cdot f'(z_0),\ \text{since $\lim_{z \to z_0}\phi(f(z)) = 0$}
\end{align*}
\end{proof}

\bigskip

\subsection{Cauchy-Riemann Equations}
%\begin{mdframed}
%\begin{center}
%{\Large Cauchy-Riemann Equations}
%\end{center}
%\end{mdframed}
%
%When considering a real-valued function $f : \rr^2 \to \rr$ of two variables, there is no notion of the derivative of a function. For such a function, we instead only have partial derivatives $f_x(x_0,y_0)$ and $f_y(x_0,y_0)$ (and also directional derivatives) which depend on the way in which we approach a point $(x_0, y_0) \in \rr^2$. For a complex-valued function $f(z)$, we now have a new concept of the derivative $f'(z_0)$, which by definition cannot depend on the way in which we approach a point $z_0 \in \cc$. It is then natural expect a relationship between the complex derivative $f'(z_0)$ and the partial derivatives of $\Re f$ and $\Im f$, which are real-values functions. 
%
%\vspace*{1em}
%
\begin{theorem}[Cauchy-Riemann Equations]\label{crequations}
Suppose that 
\[f(z) = f(x + iy) = u(x,y) + i\,v(x,y)\]
is differentiable at $z_0 = x_0 + iy_0$. Then
\begin{itemize}
\item[(a)] the first order partial derivatives of $u$ and $v$ exist at $(x_0,y_0)$ and satisfy the \cdef{Cauchy\text{-}Riemann\ Equations}
\begin{align*}\label{creqex}
u_x(x_0,y_0) &= v_y(x_0,y_0)\\[-0.5em]
\tag{CR}\\[-0.5em]
u_y(x_0,y_0) &= -v_x(x_0,y_0)
\end{align*}
\item[(b)] $f'(z_0) = u_x(x_0,y_0) + i\,v_x(x_0,y_0) = v_y(x_0,y_0) - i\,u_y(x_0,y_0)$.
\end{itemize}
\end{theorem}
\begin{proof}
Since $f$ is differentiable at $z_0$, we have, where we let $h = s + it$
\begin{align*}
f'(z_0) &= \lim_{h\to 0}\frac{f(z_0 + h) - f(z_0)}{h}\\[0.5em]
&= \lim_{h\to 0}\frac{f((x_0 + s) + i(y_0 + t)) - f(x_0 + iy_0)}{h}\\[0.5em]
&= \lim_{h\to 0}\frac{u(x_0 + s,y_0 + t) - u(x_0,y_0)}{h} + i\cdot\lim_{h\to 0}\frac{v(x_0 + s,y_0 + t) - v(x_0,y_0)}{h}
\end{align*}
As we know by now, we must get the same result if we restrict $h$ to be on the real axis and if we restrict it to be on the imaginary axis. In the former case, $t = 0$, giving us
\begin{align*}
f'(z_0) &= \lim_{s\to 0}\frac{u(x_0 + s,y_0) - u(x_0,y_0)}{s} + i\cdot\lim_{s\to 0}\frac{v(x_0 + s,y_0) - v(x_0,y_0)}{s}\\[0.5em]
&= u_x(x_0,y_0) + i\,v_x(x_0,y_0)
\end{align*}
In the latter case, $s = 0$, giving us
\begin{align*}
f'(z_0) &= \lim_{t\to 0}\frac{u(x_0,y_0 + t) - u(x_0,y_0)}{it} + i\cdot\lim_{t\to 0}\frac{v(x_0,y_0 + t) - v(x_0,y_0)}{it}\\[0.5em]
&= \frac{1}{i}\cdot\lim_{t\to 0}\frac{u(x_0,y_0 + t) - u(x_0,y_0)}{t} + \lim_{t\to 0}\frac{v(x_0,y_0 + t) - v(x_0,y_0)}{t}\\[0.5em]
&= -i\,u_y(x_0,y_0) + v_y(x_0,y_0)
\end{align*}
Therefore
\[u_x(x_0,y_0) + i\,v_x(x_0,y_0) = f'(z_0) = v_y(x_0,y_0)-i\,u_y(x_0,y_0),\]
and hence $u_x(x_0,y_0) = v_y(x_0,y_0)$ and $u_y(x_0,y_0) = -v_x(x_0,y_0)$.
\end{proof}

%\vspace*{1.5em}

The Cauchy-Riemann equations (\ref{creqex}) are a \emph{necessary} condition for $f'$ to exist. We can use them to locate possible points where the derivative does not exist but not necessarily conclude where and if the derivative exists.
\begin{example}\label{necessarycr}\hfill
\begin{itemize}
\item[(1)] Consider $f(z) = \abs{z}^2 = x^2 + y^2$, so $u(x,y) = x^2 + y^2$ and $v(x,y) = 0$. The partial derivatives at $(x,y)$ are
\begin{align*}
u_x &= 2x & v_x &= 0\\[0.5em]
u_y &= 2y & v_y &= 0
\end{align*}
Therefore, the Cauchy-Riemann equations (\ref{creqex}) are only satisfied at $(x,y) = (0,0)$. Hence $f$ is not differentiable at any $z \neq 0$. Again, note that this does not say anything about the existence of $f'(0)$.
\item[(2)] Consider $f(z) = \overline{z} = x - iy$, so $u(x,y) = x$ and $v(x,y) = -y$. The partial derivatives at $(x,y)$ are
\begin{align*}
u_x &= 1 & v_x &= 0\\[0.5em]
u_y &= 0 & v_y &= -1
\end{align*}
Note that $u_x \neq v_y$ for all $(x,y)$ and therefore the Cauchy-Riemann equations (\ref{creqex}) are satisfied for no $(x,y)$. Hence $f$ is nowhere complex-differentiable.
\item[(3)] (in-class) Consider $f(z) = (z + i\overline{z})^2$, let's simplify $f$ to identify its real and imaginary parts $u(x,y)$ and $v(x,y)$.
\begin{align*}
f(z) = f(x + iy) &= ((x + iy) + i(x - iy))^2\\[0.5em]
&= ((x + iy) + (y + ix))^2\\[0.5em]
&= ((x + y) + i(x + y))^2\\[0.5em]
&= (x + y)^2(1 + i)^2\\[0.5em]
&= (x + y)^2(1^2 + i^2 + 2i)\\[0.5em]
&= 2i(x + y)^2
\end{align*}
Therefore $u(x,y) = 0$ and $v(x,y) = 2(x + y)^2$. The partial derivatives at $(x,y)$ are
\begin{align*}
u_x &= 0 & v_x &= 4(x + y)\\[0.5em]
u_y &= 0 & v_y &= 4(x + y)
\end{align*}
Therefore, the Cauchy-Riemann equations (\ref{creqex}) are satisfied if and only if $4(x + y) = 0$, if and only if $y = -x$. Hence $f$ is not differentiable any $z \in \cc$ such that $\Im z \neq -\Re z$.
\end{itemize}
\end{example}

\vspace*{1em}

As commented, the Cauchy-Riemann equations (\ref{creqex}) are not a \emph{sufficient} condition for the existence of the derivative as the example below shows. Problem \ref{prob 7.1} gives another example.
\begin{example}\label{onlynecescr}
Consider
\[f(z) = \begin{cases}\dfrac{\overline{z}^2}{z} = \dfrac{\overline{z}^3}{\abs{z}^2} & z \neq 0\\[1em] 0 & z = 0 \end{cases}\]
Then,
\[u(x,y) = \begin{cases}\dfrac{x^3 - 3xy^2}{x^2 + y^2} & (x,y) \neq (0,0)\\[1em] 0 & (x,y) = (0,0) \end{cases} \qquad \text{and} \qquad v(x,y) = \begin{cases}\dfrac{y^3 - 3x^2y}{x^2 + y^2} & (x,y) \neq (0,0)\\[1em] 0 & (x,y) = (0,0) \end{cases}\]
We show that $u$ and $v$ satisfy the Cauchy-Riemann equations (\ref{creqex}) at $(0,0)$.
\begin{align*}
u_x(0,0) &= \lim_{s \to 0}\frac{u(s,0) - u(0,0)}{s} = \lim_{s \to 0}\frac{\dfrac{s^3}{s^2} - 0}{s} = 1\\[0.5em]
u_y(0,0) &= \lim_{t \to 0}\frac{u(0,t) - u(0,0)}{t} = \lim_{t \to 0}\frac{0 - 0}{t} = 0  \\[1em]
v_x(0,0) &= \lim_{s \to 0}\frac{v(s,0) - v(0,0)}{s} = \lim_{s \to 0}\frac{0 - 0}{s} = 0\\[0.5em]
v_y(0,0) &= \lim_{t \to 0}\frac{v(0,t) - v(0,0)}{t} = \lim_{t \to 0}\frac{\dfrac{t^3}{t^2} - 0}{t} = 1
\end{align*}
Therefore $u_x(0,0) = 1 = v_y(0,0)$ and $u_y(0,0) = 0 = -v_x(0,0)$, and hence the Cauchy-Riemann equations (\ref{creqex}) are satisfied. But $f'(0)$ does not exist, as seen in Problem \ref{prob 6.6}.
\end{example}

\vspace*{1em}

Imposing certain existence and continuity conditions on the first order partial derivatives of $u$ and $v$, the Cauchy-Riemann equations (\ref{creqex}) can be upgraded to a sufficient condition for differentiability.
\begin{theorem}[Sufficient Conditions for Differentiability]\label{crsuff}
Consider a function \[f(z) = f(x + iy) = u(x,y) + i\,v(x,y)\]
and a $z_0$ in the domain of $f$, such that
\begin{itemize}
\item[(a)] the first order partial derivatives of $u$ and $v$ exist and are continuous in an open disk centered at $z_0$; and
\item[(a)] the Cauchy-Riemann equations (\ref{creqex}) are satisfied at $(x_0,y_0)$.
\end{itemize}
Then $f'(z_0)$ exists and is given by $u_x(x_0,y_0) + i\,v_x(x_0,y_0) = v_y(x_0,y_0)-i\,u_y(x_0,y_0)$.
\end{theorem}
\begin{proof}
We skip the proof. You can find a proof in \cite[Section~22, Page~66]{brown2009complex} [1, Section 22, Page 66].
\end{proof}

\vspace*{1.5em}

\begin{example}
Let's revisit examples from Example \ref{necessarycr} and \ref{onlynecescr}.
\begin{itemize}
\item[(1)] Consider $f(z) = \abs{z}^2 = x^2 + y^2$, we noted that $u(x,y) = x^2 + y^2$ and $v(x,y) = 0$. We have seen that the only point where $f(z)$ can be differentiable is $z = 0$. The partial derivatives in a neighbourhood of $(0,0)$ are
\begin{align*}
u_x &= 2x & v_x &= 0\\[0.5em]
u_y &= 2y & v_y &= 0
\end{align*}
which clearly exist and are continuous. We have also seen that the Cauchy-Riemann equations (\ref{creqex}) are satisfied at $(0,0)$, trivially. Therefore $f'(0)$ exists and \[f'(0) = u_x(0,0) + i\,v_x(0,0) = 0.\]

\item[(2)] Consider $f(z) = (z + i\overline{z})^2$, we noted that $u(x,y) = 0$ and $v(x,y) = 2(x + y)^2$. We have seen that the only point where $f(z)$ can be differentiable are $z = x + iy \in \cc$ such that $y = \Im z = -\Re z = -x$. That is, at points of the form $(x,-x)$. The partial derivatives in a neighbourhood of $(x,-x)$ are
\begin{align*}
u_x &= 0 & v_x &= 4(x + y)\\[0.5em]
u_y &= 0 & v_y &= 4(x + y)
\end{align*}
which clearly exist and are continuous. Note the Cauchy-Riemann equations (\ref{creqex}) are satisfied at $(x,-x)$ trivially, since \[u_x(x,-x) = u_y(x,-x) = v_x(x,-x) = v_y(x,-x) = 0.\]
Therefore $f'(z)$ exists, for $z = x - ix$, and \[f'(z) = u_x(x,-x) + i\,v_x(x,-x) = 0.\]

\item[(3)] The reason Example \ref{onlynecescr} doesn't contradict Theorem \ref{crsuff} is because, $u_x$, in particular, is not continuous at $(0,0)$. Note that we have
\[u(x,y) = \begin{cases}\dfrac{x^3 - 3xy^2}{x^2 + y^2} & (x,y) \neq (0,0)\\[1em] 0 & (x,y) = (0,0) \end{cases}\]
For $(x,y) \neq (0,0)$, we compute $u_x(x,y)$ using the quotient rule, while we have already computed $u_x(0,0) = 1$ in Example \ref{onlynecescr}, giving us
\[u_x(x,y) = \begin{cases}\dfrac{x^4 + 6x^2y^2 - 3y^4}{(x^2 + y^2)^2} & (x,y) \neq (0,0)\\[1em] 1 & (x,y) = (0,0) \end{cases}\]
Suppose $u_x(x,y)$ is continuous at $(0,0)$, then we have
\[\lim_{(x,y) \to (0,0)}u_x(x,y) = \lim_{(x,y) \to (0,0)}\dfrac{x^4 + 6x^2y^2 - 3y^4}{(x^2 + y^2)^2} = u_x(0,0) = 1\]
Restricting the limit along the $y$-axis, where $x = 0$, we get
\[1 = \lim_{(0,y) \to (0,0)}\dfrac{-3y^4}{(y^2)^2} = \lim_{y \to 0}\dfrac{-3y^4}{y^4} = -3,\]
a contradiction. Hence, $u_x(x,y)$ is not continuous at $(0,0)$.
\end{itemize}
\end{example}

\vspace*{1em}

\begin{example}[Complex Exponential]\label{expcmplxeg}
Define, for any $z = x + iy \in \cc$
\[\exp(z) = e^z \coloneqq e^xe^{iy} = e^x(\cos y + i\,\sin y)\]
the \emph{complex exponential function}. Note that $e^x$ is the usual real exponential and $e^{iy}$ is given by Euler's formula (Definition \ref{eulerform}). Here, 
\[u(x,y) = e^x\cos y \quad \text{and} \quad v(x,y) = e^x\sin y\]
We then see that
\begin{align*}
u_x &= e^x\cos y = v_y,\\[0.5em] v_x &= -e^x\sin y = -u_y;
\end{align*}
so $\exp$ satisfies the Cauchy-Riemann equations (\ref{creqex}) everywhere. Furthermore, $u_x,\,u_y,\,v_x$ and $v_y$ are everywhere defined and continuous. Hence $\exp$ is everywhere complex-differentiable, an \emph{entire} function. Furthermore $\exp(z)' = u_x + iv_x = e^x\cos y + ie^x\sin y = \exp(z)$.
\end{example}

\vspace*{1em}

\begin{discussion}[Polar Cauchy-Riemann Equations]
Recall that if the domain of a function $f$ is contained in $\cc^*$ or restricted to within $\cc^*$, one can express in polar coordinates at $z = re^{i\theta}$ as
\[f(z) = f(re^{i\theta}) = u(r,\theta) + i\,v(r,\theta)\]
Then, the Cauchy-Riemann equations (\ref{creqex}) at a point $(r_0,\theta_0)$ can be expressed in polar coordinates, \cdef{Polar\ Cauchy\text{-}Riemann\ Equations} (see Problem \ref{prob 7.3})
\begin{align*}\label{pcreqex}
ru_r &= v_\theta\\[-0.5em]
\tag{Polar CR}\\[-0.5em]
u_\theta &= -rv_r
\end{align*}
and a differentiable function at $z_0 = r_0e^{i\theta_0}$ is then expressed as \[f'(z_0) = f'(r_0e^{i\theta_0}) = e^{-i\theta_0}(u_r(r_0,\theta_0) + i\,v_r(r_0,\theta_0)).\]
\end{discussion}

\vspace*{1em}

\begin{example}
Consider the function
\[f(z) = f(re^{i\theta}) = \sqrt{r}\,e^{i\frac{\theta}{2}} ,\]
where $r > 0$ and $-\pi < \theta < \pi$. This is the function that outputs the principal square root of $z$. We compute $f'(z)$ at $z = re^{i\theta}$ using the polar form of Theorem \ref{crsuff}. We first note that
\[f(z) = \underbrace{\sqrt{r}\cos\left(\frac{\theta}{2}\right)}_{u(r,\theta)} + i\underbrace{\sqrt{r}\sin\left(\frac{\theta}{2}\right)}_{v(r,\theta)}\]
Now, we compute
\begin{align*}
ru_r &= r\frac{1}{2\sqrt{r}}\cos\left(\frac{\theta}{2}\right) = \frac{\sqrt{r}}{2}\cos\left(\frac{\theta}{2}\right) = v_\theta\\[0.5em]
u_\theta &= -\frac{\sqrt{r}}{2}\sin\left(\frac{\theta}{2}\right) = -r\frac{1}{2\sqrt{r}}\sin\left(\frac{\theta}{2}\right) = -rv_r
\end{align*}
Clearly the first order partial derivatives exist everywhere and the Polar Cauchy-Riemann equations (\ref{pcreqex}) are also satisfied everywhere. Hence $f'(z)$ exists and
\begin{align*}
f'(z) &= e^{-i\theta}(u_r(r,\theta) + i\,v_r(r,\theta))\\[0.5em]
&= e^{-i\theta}\left(\frac{1}{2\sqrt{r}}\cos\left(\frac{\theta}{2}\right) + i\,\frac{1}{2\sqrt{r}}\sin\left(\frac{\theta}{2}\right)\right)\\[0.5em]
&= \frac{e^{-i\theta}}{2\sqrt{r}}\left(\cos\left(\frac{\theta}{2}\right) + i\,\sin\left(\frac{\theta}{2}\right)\right)\\[0.5em]
&= \frac{1}{2\sqrt{r}}\cdot e^{-i\theta}\cdot e^{i\frac{\theta}{2}}\\[0.5em]
&= \frac{1}{2\sqrt{r}e^{i\frac{\theta}{2}}}\\[0.5em]
&= \frac{1}{2f(z)}
\end{align*}
\end{example}

\bigskip

\subsection{Holomorphic Functions}
%\begin{mdframed}
%\begin{center}
%{\Large Holomorphic Functions}
%\end{center}
%\end{mdframed}
%
\begin{definition}[Holomorphic Functions]
A function $f$ is \emph{holomorphic on an open set $U$} if $f'(z)$ exists for every $z \in U$.\\[0.5em]
We say $f$ is \emph{holomorphic at a point $z_0$} if it holomorphic on some open disk $D_\epsilon(z_0)$ for an $\epsilon > 0$. We say $f$ is \cdef{holomorphic} if it is holomorphic at every point in its domain.\\[0.5em]
A function that is holomorphic on all of $\cc$ is said to be \cdef{entire}. 
\end{definition}

\vspace*{1em}

\begin{example}\hfill
\begin{itemize}
\item[(1)] $f(z) = \dfrac{1}{z}$ is holomorphic on any open set not containing $0$, in particular on $\cc^*$.
\item[(2)] $f(z) = \abs{z}^2$ is nowhere holomorphic since we have already seen that $f$ is only complex-differentiable at $z = 0$ and at no other point.
\item[(3)] Polynomials are entire.
\item[(4)] $f(z) = \overline{z}$ is nowhere holomorphic, since it's nowhere differentiable.
\end{itemize}
\end{example}

\vspace*{1em}

\begin{discussion}
Let $G$ be a domain (open and connected subset of $\cc$). We know several necessary and sufficient conditions for $f = u + iv$ to be holomorphic on $G$. 
\begin{itemize}[leftmargin = 6em]
\item[(Necessary)]
\begin{itemize}
\item[(1)] $f$ is continuous on $G$.
\item[(2)] Cauchy-Riemann equations (\ref{creqex}) are satisfied on $G$.
\end{itemize}
\item[(Sufficient)]
\begin{itemize}
\item[(1)] First order partial derivatives of $u$ and $v$ exist and continuous on $G$, and the Cauchy-Riemann equations (\ref{creqex}) are satisfied on $G$.
\item[(2)] Differentiation Laws. If $f$ and $g$ are holomorphic on $G$, then so are $f + g,\, fg$ and $f/g$ (if $g \neq 0$ on $G$).
\item[(3)] Composition of holomorphic functions is holomorphic.
\end{itemize}
\end{itemize}
\end{discussion}

\vspace*{1em}

\begin{theorem}[Sufficient Condition for Constantness]\label{der0const}
Suppose $G$ is a domain and $f'(z) = 0$ for all $z \in G$. Then $f(z)$ is constant on $G$.
\end{theorem}
\begin{proof}
%{\color{red}Add proof.}
Write $f(z) = f(x + iy) = u(x,y) + i\,v(x,y)$, so we have
\begin{align*}
0 = f'(z) &= u_x + iv_x = v_y - i\,u_y
\end{align*}
Therefore $u_x = u_y = 0$ and $v_x = v_y = 0$. We consider points $p,q \in G$ such that there's a line segment $L$ in $G$ connecting them. Let $\vec{w} = (a,b)$ be a unit vector parallel to $L$, then the directional derivative of $u$ along $L$ is
\[(\operatorname{grad}u)\cdot\vec{w} = au_x + bu_y = 0.\]
So, $u$ is constant along $L$. Since $G$ is a domain, any two points can be connected by a polygon line. Applying the above argument along constituent line segments, we see that $u$ has the same value along the endpoints of any polygon line. This shows that $u$ is constant on $G$, say $u(x,y) = c$. A similar argument works for $v$, giving us $v(x,y) = d$. Hence
\[f(z) = c + id,\]
that is, $f$ is constant.
\end{proof}

\vspace*{1em}

Theorem \ref{der0const} has many interesting consequences.
\begin{proposition}\label{conjholconst}
Suppose $f$ and $\bar{f}$ are holomorphic on a domain $G$. Then $f$ is constant on $G$.
\end{proposition}
\begin{proof}
We write
\begin{align*}
f(z) &= f(x + iy) = u(x,y) + i\,v(x,y)\\[0.5em]
\bar{f}(z) &= \overline{f(x + iy)} = u(x,y) - i\,v(x,y)
\end{align*}
Since $f$ and $\bar{f}$ are holomorphic, they satisfy the Cauchy-Riemann equations (\ref{creqex})
\[\text{for $f$: }\ \begin{cases}u_x = v_y\\ u_y = -v_x \end{cases}\]\\[-0.5em]
\[\text{for $\bar{f}$: }\ \begin{cases}u_x = (-v)_y = -v_y\\ u_y = -(-v)_x = v_x \end{cases}\]\\
This gives us $v_y = -v_y$ and $v_x = -v_x$, and therefore $u_x = v_x = 0$. Hence $f'(z) = u_x + i\,v_x = 0$, giving us that $f$ is constant by Theorem \ref{der0const}.
\end{proof}

\vspace*{1em}

\begin{corollary}\label{realholconst}
Suppose $f$ is holomorphic on a domain $G$ and always real-valued. Then $f$ is constant on $G$.
\end{corollary}
\begin{proof}
Since $f$ is always real-valued, we have $f = \bar{f}$. Therefore $\bar{f}$ is holomorphic on $G$ as well, and hence $f$ is constant by Proposition \ref{conjholconst}.
\end{proof}

\vspace*{1em}

\begin{corollary}\label{absholconst}
Suppose $f$ is holomorphic on a domain $G$ and $\abs{f}$ is constant on it. Then $f$ is also constant on $G$.
\end{corollary}
\begin{proof}
By assumption $\abs{f(z)} = c$, for all $z \in G$, for some $c \in \cc$. This gives us
\[f(z)\overline{f(z)} = \abs{f(z)}^2 = c^2\tag{$*$}\label{eqabs}\]
Suppose $c = 0$, then $\abs{f(z)} = 0$ and therefore $f(z) = 0$. Suppose $c \neq 0$, then necessarily $f(z) \neq 0$ for every $z \in G$ by (\ref{eqabs}). Hence
\[\overline{f(z)} = \frac{c^2}{f(z)},\]
and thus $\bar{f}$ is holomorphic. Therefore both $f$ and $\bar{f}$ are holomorphic and hence $f$ is constant by Proposition \ref{conjholconst}.
\end{proof}

\vspace*{1em}

\begin{example}
We apply Corollary \ref{absholconst} to $f(z) = \dfrac{\overline{z}}{z}$ to conclude that it's not holomorphic.\\
\\
We first note that, for any $z \in \cc$,
\[\abs{f(z)} = \abs{\frac{\overline{z}}{z}} = \frac{|\overline{z}|}{\abs{z}} = 1;\]
that is, $\abs{f}$ is constant. Suppose $f$ was holomorphic on $\cc$ (this argument can be specialised to any domain $G$), then $f$ would be a holomorphic function such that $\abs{f}$ is constant. Therefore, by Corollary \ref{absholconst}, $f$ is constant on $\cc$. That's a contradiction, since $f$ is non-constant, as $f(1) = 1$ and $f(i) = -1$. 
\end{example}

\vspace*{1em}

\begin{example}[in-class]
Is the function $f(z) = \Re z$ holomorphic?
\end{example}
\begin{proof}[Answer]
Note that $f(z) = \Re z$ is a real-valued function, for any $z \in \cc$. Suppose $f$ was holomorphic on $\cc$ (this argument can be specialised to any domain $G$), then $f$ would be a holomorphic function such that $f$ is always real-valued. Therefore, by Corollary \ref{realholconst}, $f$ is constant on $\cc$. That's a contradiction, since $f$ is non-constant, as $f(1) = 1$ and $f(i) = 0$. 
\end{proof}

\vspace*{2em}

We now discuss a large class of holomorphic functions, which are complex  versions of functions you may have seen in your Calculus classes

\vspace*{1em}

\subsection{The Exponential Function}
%\begin{mdframed}
%\begin{center}
%{\Large The Exponential Function}
%\end{center}
%\end{mdframed}
%
\begin{definition}[The Exponential Function]
The \cdef{(complex)\ exponential\, function} $e^z$ (or $\exp(z)$) is defined on all of $\cc$ as follows
\[e^z \coloneqq e^{\Re z}e^{i\Im z} = e^{\Re z}(\cos(\Im z) + i\sin(\Im z)).\]
That is, writing $z = x + iy$, we have
\[e^z = e^xe^{iy} = e^x(\cos y + i\sin y).\]
Since $x \in \rr,\  e^x$ is the usual real exponential function, while $e^{iy}$ is given by Euler's formula.\\[0.5em]
Furthermore, the definitions give us $\overline{e^z} = e^{\overline{z}}$.\\[0.5em]
Note that when $z = x \in \rr$, we have $e^z = e^x$, since then $\Im z = 0$.
\end{definition}

\vspace*{1em}

\begin{proposition}[Properties of the Exponential]\label{propexp}
Consider $z,w \in \cc$. 
\begin{itemize}
\item[(1)] $\abs{e^z} = e^{\Re z}$ and $\arg e^z = \setp{\Im z + 2k\pi}{k \in \zz}$.
\item[(2)] $e^{z + w} = e^ze^w$.
\item[(3)] $e^{z - w} = \dfrac{e^z}{e^w}$.
\item[(4)] $e^z$ is entire, and $(e^z)' = e^z$.
\item[(5)] $e^z$ is periodic: $e^{z + 2k\pi i} = e^z$ for all $k \in \zz$.
\end{itemize}
\end{proposition}
\begin{proof}\hfill
\begin{itemize}
\item[(1)] Write $z = x + iy$, then $\abs{e^z} = \abs{e^x}\abs{\cos x + i\sin x} = \abs{e^x}$. Which tells us \[\arg e^z = \setp{y + 2k\pi}{k \in \zz}.\]
\item[(2)] Write $z = x + iy$ and $w = u + iv$, then
\begin{align*}
e^{z + w} &= e^{(x+u) + i(y + v)}\\[0.5em]
&= e^{x + u}e^{i(y + v)}\\[0.5em]
&= e^x e^u e^{iy} e^{iv}\\[0.5em]
&= e^xe^{iy}e^ue^{iv}\\[0.5em]
&= e^ze^w
\end{align*}
\item[(3)] From (2) we get $e^{z-w}e^w = e^z$.
\item[(4)] This was seen in Example \ref{expcmplxeg}.
\item[(5)] From (2) we have $e^{z + 2k\pi i} = e^z e^{2k\pi i} = e^z$.
\end{itemize}
\vspace*{-\baselineskip}
\end{proof}
